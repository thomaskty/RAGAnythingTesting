<?xml version='1.0' encoding='utf-8'?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
  <key id="d11" for="edge" attr.name="created_at" attr.type="long" />
  <key id="d10" for="edge" attr.name="file_path" attr.type="string" />
  <key id="d9" for="edge" attr.name="source_id" attr.type="string" />
  <key id="d8" for="edge" attr.name="keywords" attr.type="string" />
  <key id="d7" for="edge" attr.name="description" attr.type="string" />
  <key id="d6" for="edge" attr.name="weight" attr.type="double" />
  <key id="d5" for="node" attr.name="created_at" attr.type="long" />
  <key id="d4" for="node" attr.name="file_path" attr.type="string" />
  <key id="d3" for="node" attr.name="source_id" attr.type="string" />
  <key id="d2" for="node" attr.name="description" attr.type="string" />
  <key id="d1" for="node" attr.name="entity_type" attr.type="string" />
  <key id="d0" for="node" attr.name="entity_id" attr.type="string" />
  <graph edgedefault="undirected">
    <node id="Synthesis Stage">
      <data key="d0">Synthesis Stage</data>
      <data key="d1">concept</data>
      <data key="d2">The Synthesis Stage combines retrieved multimodal knowledge into coherent, evidence-grounded responses for effective question answering.&lt;SEP&gt;The Synthesis Stage addresses challenges by systematically combining retrieved multimodal knowledge into comprehensive, evidence-grounded responses.</data>
      <data key="d3">chunk-50ef79f5820dd68dd3803e9eecf98e80&lt;SEP&gt;chunk-36eedc88c32f26a4209663bcf9de5e48</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918067</data>
    </node>
    <node id="Textual Context">
      <data key="d0">Textual Context</data>
      <data key="d1">concept</data>
      <data key="d2">Textual Context is constructed from top-ranked retrieval candidates to provide a structured format that facilitates understanding of heterogeneous knowledge components.</data>
      <data key="d3">chunk-36eedc88c32f26a4209663bcf9de5e48</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918067</data>
    </node>
    <node id="Visual Content">
      <data key="d0">Visual Content</data>
      <data key="d1">concept</data>
      <data key="d2">Visual Content is recovered through dereferencing in multimodal chunks, maintaining consistency with a unified embedding strategy.</data>
      <data key="d3">chunk-36eedc88c32f26a4209663bcf9de5e48</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918067</data>
    </node>
    <node id="Vision-Language Model">
      <data key="d0">Vision-Language Model</data>
      <data key="d1">method</data>
      <data key="d2">The Vision-Language Model integrates information from queries, textual context, and visual content to enable sophisticated visual interpretation.</data>
      <data key="d3">chunk-36eedc88c32f26a4209663bcf9de5e48</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918067</data>
    </node>
    <node id="DocBench">
      <data key="d0">DocBench</data>
      <data key="d1">data</data>
      <data key="d2">DocBench is one of the multimodal benchmarks used for comprehensive experiments to validate the effectiveness of the RAG-Anything framework.&lt;SEP&gt;DocBench is a multimodal Document Question Answering benchmark featuring 229 documents across five domains, designed for evaluating long-context understanding.&lt;SEP&gt;DocBench is a dataset used for evaluating model performance across varying document lengths.&lt;SEP&gt;DocBench is an experimental dataset consisting of 229 documents, characterized by a high average token count of 46,377 and 1,102 associated questions.&lt;SEP&gt;DocBench is an evaluation benchmark used for assessing various document retrieval methods based on performance metrics.</data>
      <data key="d3">chunk-36eedc88c32f26a4209663bcf9de5e48&lt;SEP&gt;chunk-ea853774b3241b5f39f26edf475d4bd5&lt;SEP&gt;chunk-260174f5a5690a7e51bdf14e210da8b4&lt;SEP&gt;chunk-3dd00be716f65d39ed28040e8449c044&lt;SEP&gt;chunk-2342fe853447858443171ce27b6bf554</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918200</data>
    </node>
    <node id="MMLongBench">
      <data key="d0">MMLongBench</data>
      <data key="d1">concept</data>
      <data key="d2">MMLongBench is the second multimodal benchmark for testing the performance of the RAG-Anything framework.&lt;SEP&gt;MMLongBench focuses on long-context multimodal document comprehension with 135 documents and 1,082 expert-annotated questions.&lt;SEP&gt;MMLongBench is another dataset used to evaluate document retrieval models, demonstrating RAG-Anything's effectiveness across different document lengths.&lt;SEP&gt;MMLongBench is an experimental dataset with 135 documents, featuring an average token count of 21,214 and 1,082 associated questions, focused on varying complexity across fewer documents.&lt;SEP&gt;MMLongBench is a benchmark for evaluating performance on multimodal document understanding tasks across various domains.</data>
      <data key="d3">chunk-36eedc88c32f26a4209663bcf9de5e48&lt;SEP&gt;chunk-bed5ba3a860481b1c29ee139ebb94960&lt;SEP&gt;chunk-ea853774b3241b5f39f26edf475d4bd5&lt;SEP&gt;chunk-3dd00be716f65d39ed28040e8449c044&lt;SEP&gt;chunk-2342fe853447858443171ce27b6bf554</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918198</data>
    </node>
    <node id="GPT-4o-mini">
      <data key="d0">GPT-4o-mini</data>
      <data key="d1">method</data>
      <data key="d2">GPT-4o-mini is a multimodal language model used as a baseline for performance evaluation in understanding long-context documents.&lt;SEP&gt;GPT-4o-mini is a multimodal document processing method that showed varying accuracy scores across different domains, with a notable low of 43.8% in Unanswerable queries.</data>
      <data key="d3">chunk-c74c755aa200873b54b093c4aef3485e&lt;SEP&gt;chunk-36eedc88c32f26a4209663bcf9de5e48</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918194</data>
    </node>
    <node id="LightRAG">
      <data key="d0">LightRAG</data>
      <data key="d1">method</data>
      <data key="d2">LightRAG is a graph-enhanced RAG system that integrates structured knowledge representation with dual-level retrieval mechanisms.&lt;SEP&gt;LightRAG is a multimodal document processing method that demonstrated competitive accuracy in several domains, particularly scoring 85.0% in the Text-only category.&lt;SEP&gt;LightRAG is another model assessed in the MMLongBench evaluation, providing accuracy metrics across different domains.</data>
      <data key="d3">chunk-c74c755aa200873b54b093c4aef3485e&lt;SEP&gt;chunk-36eedc88c32f26a4209663bcf9de5e48&lt;SEP&gt;chunk-bed5ba3a860481b1c29ee139ebb94960</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918195</data>
    </node>
    <node id="MMGraphRAG">
      <data key="d0">MMGraphRAG</data>
      <data key="d1">method</data>
      <data key="d2">MMGraphRAG is a multimodal retrieval framework that constructs unified knowledge graphs for entity analysis and retrieval guidance.&lt;SEP&gt;MMGraphRAG is another model that exhibits comparable performance to RAG-Anything on shorter documents and is part of a comparative analysis in document retrieval.&lt;SEP&gt;MMGraphRAG is a method that links scene graphs with textual representations but struggles with structural blindness, particularly in processing complex tables and formulas.&lt;SEP&gt;MMGraphRAG is a data series in the 'DocBench Accuracy' chart, showing lower accuracy compared to 'RAGAnything'.&lt;SEP&gt;MMGraphRAG is a multimodal document processing technique with diverse accuracy scores, excelling in the Government domain with 64.9%.&lt;SEP&gt;MMGraphRAG is a multimodal model evaluated against other methods on the MMLongBench benchmark.</data>
      <data key="d3">chunk-bed5ba3a860481b1c29ee139ebb94960&lt;SEP&gt;chunk-0ed9b2ed0a4bc6dedef04c5c1a224400&lt;SEP&gt;chunk-36eedc88c32f26a4209663bcf9de5e48&lt;SEP&gt;chunk-b05cce72ca9f7c1081afa465d0eb6116&lt;SEP&gt;chunk-ea853774b3241b5f39f26edf475d4bd5&lt;SEP&gt;chunk-c74c755aa200873b54b093c4aef3485e</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918194</data>
    </node>
    <node id="RAG-Anything">
      <data key="d0">RAG-Anything</data>
      <data key="d1">method</data>
      <data key="d2">RAG-Anything is a sophisticated system and proposed framework aimed at addressing the complexities associated with multimodal knowledge representation and retrieval. It employs dual-graph construction strategies, which enhance both the understanding and retrieval of diverse modalities. This unified framework supports the retrieval process across various types of information, making it applicable to a wide range of contexts.

In addition to its foundational purpose, RAG-Anything also functions as a hybrid retrieval architecture that merges structural knowledge navigation with semantic similarity matching. This combination is particularly effective for document retrieval tasks, enabling enhanced document analysis and navigation. A specific application noted for RAG-Anything is its capability to transform financial report tables into structured graphs, facilitating improved insights from typically complex data formats.

RAG-Anything has demonstrated superior performance in multimodal document understanding, notably achieving top results in the MMLongBench evaluation, which assesses models based on their efficacy across multiple document categories. Its effectiveness in these categories reflects its advanced capabilities over other methods in the multimodal domain, establishing RAG-Anything as a leading model in the field of multimodal knowledge representation and indexing.</data>
      <data key="d3">chunk-36eedc88c32f26a4209663bcf9de5e48&lt;SEP&gt;chunk-bed5ba3a860481b1c29ee139ebb94960&lt;SEP&gt;chunk-b05cce72ca9f7c1081afa465d0eb6116&lt;SEP&gt;chunk-ea853774b3241b5f39f26edf475d4bd5&lt;SEP&gt;chunk-92698bb9a611d4820a4578e37e74e670&lt;SEP&gt;chunk-260174f5a5690a7e51bdf14e210da8b4&lt;SEP&gt;chunk-fb3b2a9642d9402ce461122c98a81fd6&lt;SEP&gt;chunk-3dd00be716f65d39ed28040e8449c044&lt;SEP&gt;chunk-47439f39943c500ebbd82430fcd5a47f&lt;SEP&gt;chunk-2342fe853447858443171ce27b6bf554</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918198</data>
    </node>
    <node id="Carbon-Fiber Spikes">
      <data key="d0">Carbon-Fiber Spikes</data>
      <data key="d1">equipment</data>
      <data key="d2">Carbon-fiber spikes are advanced sprinting shoes used to enhance performance in sprints.&lt;SEP&gt;Carbon-Fiber Spikes are specialized footwear used to enhance athletic performance in sprinting, which could relate to methodology in model evaluation.</data>
      <data key="d3">chunk-36eedc88c32f26a4209663bcf9de5e48&lt;SEP&gt;chunk-bed5ba3a860481b1c29ee139ebb94960</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918199</data>
    </node>
    <node id="Financial Table Navigation">
      <data key="d0">Financial Table Navigation</data>
      <data key="d1">method</data>
      <data key="d2">Financial Table Navigation is a method that addresses challenges in extracting metrics from financial tables with ambiguous terms and multiple time periods.</data>
      <data key="d3">chunk-b05cce72ca9f7c1081afa465d0eb6116</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918068</data>
    </node>
    <node id="Key Insights">
      <data key="d0">Key Insights</data>
      <data key="d1">concept</data>
      <data key="d2">Key Insights refers to the understanding derived from the application of RAG-Anything's structure-aware design in document processing.</data>
      <data key="d3">chunk-b05cce72ca9f7c1081afa465d0eb6116</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918068</data>
    </node>
    <node id="Graph-Enhanced Retrieval-Augmented Generation">
      <data key="d0">Graph-Enhanced Retrieval-Augmented Generation</data>
      <data key="d1">concept</data>
      <data key="d2">Graph-Enhanced Retrieval-Augmented Generation refers to the framework that enhances retrieval efficiency and accuracy through relational modeling.</data>
      <data key="d3">chunk-b05cce72ca9f7c1081afa465d0eb6116</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918068</data>
    </node>
    <node id="GraphRAG">
      <data key="d0">GraphRAG</data>
      <data key="d1">method</data>
      <data key="d2">GraphRAG is a method that introduced graph structures to improve the retrieval process in large language models.</data>
      <data key="d3">chunk-b05cce72ca9f7c1081afa465d0eb6116</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918069</data>
    </node>
    <node id="Multimodal Retrieval-Augmented Generation">
      <data key="d0">Multimodal Retrieval-Augmented Generation</data>
      <data key="d1">method</data>
      <data key="d2">Multimodal Retrieval-Augmented Generation represents an evolution from text-based systems that integrates knowledge from various types of data.</data>
      <data key="d3">chunk-b05cce72ca9f7c1081afa465d0eb6116</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918069</data>
    </node>
    <node id="Appendix A.2">
      <data key="d0">Appendix A.2</data>
      <data key="d1">content</data>
      <data key="d2">Appendix A.2 includes additional case studies related to the capabilities and applications of RAG-Anything in document processing.</data>
      <data key="d3">chunk-b05cce72ca9f7c1081afa465d0eb6116</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918069</data>
    </node>
    <node id="Appendix A.5">
      <data key="d0">Appendix A.5</data>
      <data key="d1">content</data>
      <data key="d2">Appendix A.5 contains an analysis revealing challenges facing current multimodal RAG systems.&lt;SEP&gt;Appendix A.5 reveals challenges faced by current multimodal RAG systems through failure case examination.</data>
      <data key="d3">chunk-b05cce72ca9f7c1081afa465d0eb6116&lt;SEP&gt;chunk-8a30975c53ad9871d2711abf552b151f</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918069</data>
    </node>
    <node id="Edge et al.">
      <data key="d0">Edge et al.</data>
      <data key="d1">person</data>
      <data key="d2">Edge et al. are researchers who contributed to the development of GraphRAG.</data>
      <data key="d3">chunk-b05cce72ca9f7c1081afa465d0eb6116</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918069</data>
    </node>
    <node id="Zhang et al.">
      <data key="d0">Zhang et al.</data>
      <data key="d1">person</data>
      <data key="d2">Zhang et al. are referenced as researchers contributing to the concepts surrounding Large Language Models and RAG frameworks.&lt;SEP&gt;Zhang et al. conducted research on the limitations of large language models with long-context inputs.</data>
      <data key="d3">chunk-b05cce72ca9f7c1081afa465d0eb6116&lt;SEP&gt;chunk-47439f39943c500ebbd82430fcd5a47f</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918069</data>
    </node>
    <node id="Bei et al.">
      <data key="d0">Bei et al.</data>
      <data key="d1">person</data>
      <data key="d2">Bei et al. explored graph structures that improve retrieval efficiency and reasoning accuracy in document systems.</data>
      <data key="d3">chunk-b05cce72ca9f7c1081afa465d0eb6116</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918070</data>
    </node>
    <node id="Guo et al.">
      <data key="d0">Guo et al.</data>
      <data key="d1">person</data>
      <data key="d2">Guo et al. developed LightRAG, focusing on optimizing graph structures for retrieval efficiency.</data>
      <data key="d3">chunk-b05cce72ca9f7c1081afa465d0eb6116</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918070</data>
    </node>
    <node id="Mavromatis &amp; Karypis">
      <data key="d0">Mavromatis &amp; Karypis</data>
      <data key="d1">person</data>
      <data key="d2">Mavromatis &amp; Karypis worked on neural models like GNN-RAG for enhancing retrieval in document processing.</data>
      <data key="d3">chunk-b05cce72ca9f7c1081afa465d0eb6116</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918070</data>
    </node>
    <node id="Jimenez Gutierrez et al.">
      <data key="d0">Jimenez Gutierrez et al.</data>
      <data key="d1">person</data>
      <data key="d2">Jimenez Gutierrez et al. created HippoRAG, a memory-augmented variant for improved retrieval performance.</data>
      <data key="d3">chunk-b05cce72ca9f7c1081afa465d0eb6116</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918070</data>
    </node>
    <node id="Sarthi et al.">
      <data key="d0">Sarthi et al.</data>
      <data key="d1">person</data>
      <data key="d2">Sarthi et al. developed hierarchical methods like RAPTOR, integrating information for multi-level reasoning.</data>
      <data key="d3">chunk-b05cce72ca9f7c1081afa465d0eb6116</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918070</data>
    </node>
    <node id="Wang et al.">
      <data key="d0">Wang et al.</data>
      <data key="d1">person</data>
      <data key="d2">Wang et al. worked on ArchRAG, contributing to knowledge aggregation in retrieval-augmented generation systems.</data>
      <data key="d3">chunk-b05cce72ca9f7c1081afa465d0eb6116</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918070</data>
    </node>
    <node id="VideoRAG">
      <data key="d0">VideoRAG</data>
      <data key="d1">method</data>
      <data key="d2">VideoRAG is a methodology employing dual-channel architectures for video understanding, which may lose critical visual information when converting videos to text.</data>
      <data key="d3">chunk-b05cce72ca9f7c1081afa465d0eb6116</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918070</data>
    </node>
    <node id="VisRAG">
      <data key="d0">VisRAG</data>
      <data key="d1">method</data>
      <data key="d2">VisRAG is a system that preserves document layouts as images but fails to accurately capture granular relationships within the content.</data>
      <data key="d3">chunk-b05cce72ca9f7c1081afa465d0eb6116</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918070</data>
    </node>
    <node id="Multimodal Knowledge Graph">
      <data key="d0">Multimodal Knowledge Graph</data>
      <data key="d1">concept</data>
      <data key="d2">The Multimodal Knowledge Graph encompasses knowledge derived from non-textual modalities such as images, tables, and equations, represented as structured graph entities.&lt;SEP&gt;Multimodal Knowledge Graph is a framework that integrates diverse data types while maintaining contextual integrity during retrieval processes.</data>
      <data key="d3">chunk-fb3b2a9642d9402ce461122c98a81fd6&lt;SEP&gt;chunk-1b0413a81536bbb879f772d32b8c0dfe</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918203</data>
    </node>
    <node id="Text-Based Knowledge Graph">
      <data key="d0">Text-Based Knowledge Graph</data>
      <data key="d1">artifact</data>
      <data key="d2">The Text-Based Knowledge Graph is constructed using established methodologies to capture explicit knowledge and semantic connections from textual content.&lt;SEP&gt;The Text-Based Knowledge Graph represents data through nodes like 'Beekeeper' and 'Observer'.</data>
      <data key="d3">chunk-92698bb9a611d4820a4578e37e74e670&lt;SEP&gt;chunk-fb3b2a9642d9402ce461122c98a81fd6</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918196</data>
    </node>
    <node id="Cross-Modal Knowledge Graph">
      <data key="d0">Cross-Modal Knowledge Graph</data>
      <data key="d1">artifact</data>
      <data key="d2">The Cross-Modal Knowledge Graph grounds non-textual modalities within their contextual environment, enabling rich semantic integration.&lt;SEP&gt;The Cross-Modal Knowledge Graph visualizes data through nodes labeled as 'Parent Node' and 'Child Node'.</data>
      <data key="d3">chunk-92698bb9a611d4820a4578e37e74e670&lt;SEP&gt;chunk-fb3b2a9642d9402ce461122c98a81fd6</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918196</data>
    </node>
    <node id="Entity Alignment">
      <data key="d0">Entity Alignment</data>
      <data key="d1">method</data>
      <data key="d2">Entity Alignment is a process that merges multiple graph structures by identifying semantically equivalent entities across them.</data>
      <data key="d3">chunk-fb3b2a9642d9402ce461122c98a81fd6</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918070</data>
    </node>
    <node id="Graph Fusion">
      <data key="d0">Graph Fusion</data>
      <data key="d1">method</data>
      <data key="d2">Graph Fusion combines different graph representations to create a unified knowledge representation leveraging multidimensional document insights.</data>
      <data key="d3">chunk-fb3b2a9642d9402ce461122c98a81fd6</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918071</data>
    </node>
    <node id="Embedding Table">
      <data key="d0">Embedding Table</data>
      <data key="d1">data</data>
      <data key="d2">The Embedding Table is a comprehensive structure that encodes dense representations for graph entities, relationships, and content chunks for efficient retrieval.&lt;SEP&gt;The Embedding Table is part of the retrieval index that facilitates efficient representations of knowledge in dense vector space.</data>
      <data key="d3">chunk-50ef79f5820dd68dd3803e9eecf98e80&lt;SEP&gt;chunk-fb3b2a9642d9402ce461122c98a81fd6</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918071</data>
    </node>
    <node id="Multimodal Large Language Models">
      <data key="d0">Multimodal Large Language Models</data>
      <data key="d1">concept</data>
      <data key="d2">Multimodal Large Language Models are utilized to derive representations from atomic content units in the context of RAG-Anything's knowledge processing approach.</data>
      <data key="d3">chunk-fb3b2a9642d9402ce461122c98a81fd6</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918071</data>
    </node>
    <node id="Local Neighborhood">
      <data key="d0">Local Neighborhood</data>
      <data key="d1">concept</data>
      <data key="d2">The Local Neighborhood is a contextual reference used in the entity representation generation process, providing localized context for each unit.</data>
      <data key="d3">chunk-fb3b2a9642d9402ce461122c98a81fd6</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918071</data>
    </node>
    <node id="Multimodal RAG Systems">
      <data key="d0">Multimodal RAG Systems</data>
      <data key="d1">concept</data>
      <data key="d2">Multimodal RAG systems refer to retrieval-augmented generation systems that integrate various types of information, facing significant challenges in current implementations.</data>
      <data key="d3">chunk-8a30975c53ad9871d2711abf552b151f</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918071</data>
    </node>
    <node id="Text-Centric Retrieval Bias">
      <data key="d0">Text-Centric Retrieval Bias</data>
      <data key="d1">concept</data>
      <data key="d2">Text-centric retrieval bias is a fundamental issue in which systems preferentially access textual sources over visual information in response to queries.</data>
      <data key="d3">chunk-8a30975c53ad9871d2711abf552b151f</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918071</data>
    </node>
    <node id="Cross-Modal Misalignment">
      <data key="d0">Cross-Modal Misalignment</data>
      <data key="d1">concept</data>
      <data key="d2">Cross-modal misalignment refers to scenarios where there is a mismatch between different types of information sources in multimodal systems.</data>
      <data key="d3">chunk-8a30975c53ad9871d2711abf552b151f</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918071</data>
    </node>
    <node id="Adaptive Spatial Reasoning">
      <data key="d0">Adaptive Spatial Reasoning</data>
      <data key="d1">concept</data>
      <data key="d2">Adaptive spatial reasoning is a proposed mechanism to improve handling of non-standard layouts in multimodal document processing.</data>
      <data key="d3">chunk-8a30975c53ad9871d2711abf552b151f</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918071</data>
    </node>
    <node id="Layout-Aware Parsing Mechanisms">
      <data key="d0">Layout-Aware Parsing Mechanisms</data>
      <data key="d1">concept</data>
      <data key="d2">Layout-aware parsing mechanisms are needed to adapt to complex document structures in real-world scenarios.</data>
      <data key="d3">chunk-8a30975c53ad9871d2711abf552b151f</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918071</data>
    </node>
    <node id="Information Landscapes">
      <data key="d0">Information Landscapes</data>
      <data key="d1">concept</data>
      <data key="d2">Information landscapes refer to the heterogeneous nature of diverse information that is analyzed within the context of multimodal RAG systems.</data>
      <data key="d3">chunk-8a30975c53ad9871d2711abf552b151f</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918071</data>
    </node>
    <node id="Multimodal Knowledge Unification">
      <data key="d0">Multimodal Knowledge Unification</data>
      <data key="d1">method</data>
      <data key="d2">Multimodal Knowledge Unification is a process introduced by RAG-Anything that decomposes heterogeneous content into atomic knowledge units while preserving structural context and semantic alignment.&lt;SEP&gt;Multimodal Knowledge Unification is a process that integrates input documents in various formats to create a structured content list.</data>
      <data key="d3">chunk-92698bb9a611d4820a4578e37e74e670&lt;SEP&gt;chunk-2342fe853447858443171ce27b6bf554</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918195</data>
    </node>
    <node id="Cross-Modal Hybrid Retrieval">
      <data key="d0">Cross-Modal Hybrid Retrieval</data>
      <data key="d1">method</data>
      <data key="d2">Cross-Modal Hybrid Retrieval is a framework introduced to address challenges in retrieving multimodal documents by leveraging structural knowledge and semantic representations.&lt;SEP&gt;Cross-Modal Hybrid Retrieval is a mechanism utilized by RAG-Anything that combines structural knowledge navigation with semantic similarity matching.&lt;SEP&gt;Cross-Modal Hybrid Retrieval is an approach that integrates different types of knowledge navigation and semantic matching across modalities.</data>
      <data key="d3">chunk-47439f39943c500ebbd82430fcd5a47f&lt;SEP&gt;chunk-50ef79f5820dd68dd3803e9eecf98e80&lt;SEP&gt;chunk-2342fe853447858443171ce27b6bf554</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918071</data>
    </node>
    <node id="Large Language Models (LLMs)">
      <data key="d0">Large Language Models (LLMs)</data>
      <data key="d1">concept</data>
      <data key="d2">Large Language Models represent a class of AI models that can benefit from RAG systems for incorporating external knowledge and enhancing reasoning abilities.</data>
      <data key="d3">chunk-2342fe853447858443171ce27b6bf554</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918072</data>
    </node>
    <node id="Current RAG Systems">
      <data key="d0">Current RAG Systems</data>
      <data key="d1">concept</data>
      <data key="d2">Current RAG Systems are existing frameworks that primarily rely on text-only approaches for knowledge retrieval, posing limitations in real-world deployment.</data>
      <data key="d3">chunk-2342fe853447858443171ce27b6bf554</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918072</data>
    </node>
    <node id="Structured Data">
      <data key="d0">Structured Data</data>
      <data key="d1">concept</data>
      <data key="d2">Structured Data refers to any organized information format that can be utilized by RAG-Anything for effective retrieval.</data>
      <data key="d3">chunk-2342fe853447858443171ce27b6bf554</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918072</data>
    </node>
    <node id="Retrieval-Augmented Generation (RAG)">
      <data key="d0">Retrieval-Augmented Generation (RAG)</data>
      <data key="d1">concept</data>
      <data key="d2">Retrieval-Augmented Generation (RAG) is a paradigm that enhances the knowledge boundaries of Large Language Models (LLMs) by allowing them to retrieve and incorporate external knowledge during inference.&lt;SEP&gt;Retrieval-Augmented Generation (RAG) is a paradigm for enhancing Large Language Models with dynamic knowledge retrieval capabilities.</data>
      <data key="d3">chunk-47439f39943c500ebbd82430fcd5a47f&lt;SEP&gt;chunk-2342fe853447858443171ce27b6bf554</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918072</data>
    </node>
    <node id="Graph Relationships">
      <data key="d0">Graph Relationships</data>
      <data key="d1">concept</data>
      <data key="d2">Graph Relationships refer to the explicit connections captured by RAG-Anything that facilitate multi-hop reasoning patterns for effective information retrieval.</data>
      <data key="d3">chunk-2342fe853447858443171ce27b6bf554</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918072</data>
    </node>
    <node id="Multimodal Reality">
      <data key="d0">Multimodal Reality</data>
      <data key="d1">concept</data>
      <data key="d2">Multimodal Reality highlights the diverse and heterogeneous nature of real-world knowledge repositories that combine text, images, data, and more.</data>
      <data key="d3">chunk-2342fe853447858443171ce27b6bf554</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918072</data>
    </node>
    <node id="Graph Construction">
      <data key="d0">Graph Construction</data>
      <data key="d1">method</data>
      <data key="d2">Graph construction is a key method in RAG-Anything that facilitates the capturing of structural and cross-modal relationships for multimodal documents.</data>
      <data key="d3">chunk-ea853774b3241b5f39f26edf475d4bd5</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918072</data>
    </node>
    <node id="Ablation Studies">
      <data key="d0">Ablation Studies</data>
      <data key="d1">method</data>
      <data key="d2">Ablation studies are systematic evaluations conducted to assess the contributions of architectural components within the RAG-Anything framework.</data>
      <data key="d3">chunk-ea853774b3241b5f39f26edf475d4bd5</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918072</data>
    </node>
    <node id="Chunk-only">
      <data key="d0">Chunk-only</data>
      <data key="d1">method</data>
      <data key="d2">Chunk-only is a variant of the retrieval method that relies solely on traditional chunk-based retrieval, bypassing graph construction.</data>
      <data key="d3">chunk-ea853774b3241b5f39f26edf475d4bd5</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918072</data>
    </node>
    <node id="w/o Reranker">
      <data key="d0">w/o Reranker</data>
      <data key="d1">method</data>
      <data key="d2">w/o Reranker refers to the retrieval approach that removes the cross-modal reranking component while keeping the graph-based architecture intact.</data>
      <data key="d3">chunk-ea853774b3241b5f39f26edf475d4bd5</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918072</data>
    </node>
    <node id="Case Studies">
      <data key="d0">Case Studies</data>
      <data key="d1">concept</data>
      <data key="d2">Case studies in the context of RAG-Anything are analyses of representative examples that illustrate how the model leverages structural information in multimodal documents.</data>
      <data key="d3">chunk-ea853774b3241b5f39f26edf475d4bd5</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918072</data>
    </node>
    <node id="Novo Nordisk">
      <data key="d0">Novo Nordisk</data>
      <data key="d1">organization</data>
      <data key="d2">Novo Nordisk is a company mentioned in the context of financial analysis, focusing on their wages and salaries expenditures.</data>
      <data key="d3">chunk-ea853774b3241b5f39f26edf475d4bd5</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918072</data>
    </node>
    <node id="Figure 2">
      <data key="d0">Figure 2</data>
      <data key="d1">concept</data>
      <data key="d2">Figure 2 is a visual representation used in the text to display model performance comparisons and style space separations.</data>
      <data key="d3">chunk-ea853774b3241b5f39f26edf475d4bd5</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918072</data>
    </node>
    <node id="Figure 3">
      <data key="d0">Figure 3</data>
      <data key="d1">content</data>
      <data key="d2">Figure 3 illustrates a multi-panel visualization commonly used in academic literature to present experimental results.&lt;SEP&gt;Figure 3 is a reference within the text that illustrates the multi-panel figure interpretation case.</data>
      <data key="d3">chunk-ea853774b3241b5f39f26edf475d4bd5&lt;SEP&gt;chunk-76f41ba1f43d29b4684e8f0113c3efae</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918197</data>
    </node>
    <node id="Figure 4">
      <data key="d0">Figure 4</data>
      <data key="d1">concept</data>
      <data key="d2">Figure 4 depicts a financial table navigation case, highlighting the querying of specific metrics in financial documents.</data>
      <data key="d3">chunk-ea853774b3241b5f39f26edf475d4bd5</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918073</data>
    </node>
    <node id="Retrieval Stage">
      <data key="d0">Retrieval Stage</data>
      <data key="d1">concept</data>
      <data key="d2">The Retrieval Stage operates on the index to identify relevant knowledge components in response to user queries.</data>
      <data key="d3">chunk-50ef79f5820dd68dd3803e9eecf98e80</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918073</data>
    </node>
    <node id="User Query">
      <data key="d0">User Query</data>
      <data key="d1">data</data>
      <data key="d2">A User Query is the input provided by the user, which is analyzed for lexical cues to enhance retrieval effectiveness.</data>
      <data key="d3">chunk-50ef79f5820dd68dd3803e9eecf98e80</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918073</data>
    </node>
    <node id="Modality-Aware Query Encoding">
      <data key="d0">Modality-Aware Query Encoding</data>
      <data key="d1">method</data>
      <data key="d2">Modality-Aware Query Encoding is a process that analyzes user queries to extract modality preferences and perform unified text embedding for cross-modal retrieval.</data>
      <data key="d3">chunk-50ef79f5820dd68dd3803e9eecf98e80</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918073</data>
    </node>
    <node id="Hybrid Knowledge Retrieval Architecture">
      <data key="d0">Hybrid Knowledge Retrieval Architecture</data>
      <data key="d1">concept</data>
      <data key="d2">Hybrid Knowledge Retrieval Architecture combines structural knowledge navigation and semantic similarity matching to enhance knowledge retrieval accuracy.</data>
      <data key="d3">chunk-50ef79f5820dd68dd3803e9eecf98e80</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918073</data>
    </node>
    <node id="Structural Knowledge Navigation">
      <data key="d0">Structural Knowledge Navigation</data>
      <data key="d1">method</data>
      <data key="d2">Structural Knowledge Navigation is a mechanism to capture explicit relationships and multi-hop reasoning patterns during the retrieval process.</data>
      <data key="d3">chunk-50ef79f5820dd68dd3803e9eecf98e80</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918073</data>
    </node>
    <node id="Semantic Similarity Matching">
      <data key="d0">Semantic Similarity Matching</data>
      <data key="d1">method</data>
      <data key="d2">Semantic Similarity Matching identifies semantically relevant knowledge that may not have explicit structural connections.</data>
      <data key="d3">chunk-50ef79f5820dd68dd3803e9eecf98e80</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918073</data>
    </node>
    <node id="Candidate Pool">
      <data key="d0">Candidate Pool</data>
      <data key="d1">concept</data>
      <data key="d2">The Candidate Pool comprises retrieved candidates from both structural navigation and semantic matching pathways, unified for final ranking.</data>
      <data key="d3">chunk-50ef79f5820dd68dd3803e9eecf98e80</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918073</data>
    </node>
    <node id="Multi-Signal Fusion Scoring">
      <data key="d0">Multi-Signal Fusion Scoring</data>
      <data key="d1">method</data>
      <data key="d2">Multi-Signal Fusion Scoring is a mechanism that integrates various relevance signals to rank retrieval candidates effectively.</data>
      <data key="d3">chunk-50ef79f5820dd68dd3803e9eecf98e80</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918073</data>
    </node>
    <node id="Hybrid Retrieval Integration">
      <data key="d0">Hybrid Retrieval Integration</data>
      <data key="d1">method</data>
      <data key="d2">Hybrid Retrieval Integration leverages the strengths of both knowledge graphs and dense representations to enhance multimodal knowledge coverage.</data>
      <data key="d3">chunk-50ef79f5820dd68dd3803e9eecf98e80</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918073</data>
    </node>
    <node id="Knowledge Graph">
      <data key="d0">Knowledge Graph</data>
      <data key="d1">concept</data>
      <data key="d2">The Knowledge Graph encodes structural properties that aid in the retrieval process by establishing explicit relationships between entities.</data>
      <data key="d3">chunk-50ef79f5820dd68dd3803e9eecf98e80</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918073</data>
    </node>
    <node id="Unified Knowledge Graph">
      <data key="d0">Unified Knowledge Graph</data>
      <data key="d1">concept</data>
      <data key="d2">The Unified Knowledge Graph combines various modalities and sources of knowledge for enhanced retrieval consistency and effectiveness.</data>
      <data key="d3">chunk-50ef79f5820dd68dd3803e9eecf98e80</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918074</data>
    </node>
    <node id="Cosine Similarity">
      <data key="d0">Cosine Similarity</data>
      <data key="d1">method</data>
      <data key="d2">Cosine Similarity is a metric used to rank semantically similar candidate chunks based on their vector representations during the retrieval process.</data>
      <data key="d3">chunk-50ef79f5820dd68dd3803e9eecf98e80</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918074</data>
    </node>
    <node id="Zirui Guo">
      <data key="d0">Zirui Guo</data>
      <data key="d1">person</data>
      <data key="d2">Zirui Guo is an author and researcher affiliated with The University of Hong Kong, contributing to the development of RAG-Anything.</data>
      <data key="d3">chunk-47439f39943c500ebbd82430fcd5a47f</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918074</data>
    </node>
    <node id="Xubin Ren">
      <data key="d0">Xubin Ren</data>
      <data key="d1">person</data>
      <data key="d2">Xubin Ren is an author and researcher affiliated with The University of Hong Kong, contributing to the development of RAG-Anything.</data>
      <data key="d3">chunk-47439f39943c500ebbd82430fcd5a47f</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918074</data>
    </node>
    <node id="Lingrui Xu">
      <data key="d0">Lingrui Xu</data>
      <data key="d1">person</data>
      <data key="d2">Lingrui Xu is an author and researcher affiliated with The University of Hong Kong, contributing to the development of RAG-Anything.</data>
      <data key="d3">chunk-47439f39943c500ebbd82430fcd5a47f</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918074</data>
    </node>
    <node id="Jiahao Zhang">
      <data key="d0">Jiahao Zhang</data>
      <data key="d1">person</data>
      <data key="d2">Jiahao Zhang is an author and researcher affiliated with The University of Hong Kong, contributing to the development of RAG-Anything.</data>
      <data key="d3">chunk-47439f39943c500ebbd82430fcd5a47f</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918074</data>
    </node>
    <node id="Chao Huang">
      <data key="d0">Chao Huang</data>
      <data key="d1">person</data>
      <data key="d2">Chao Huang is an author and researcher affiliated with The University of Hong Kong, contributing to the development of RAG-Anything.</data>
      <data key="d3">chunk-47439f39943c500ebbd82430fcd5a47f</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918074</data>
    </node>
    <node id="The University of Hong Kong">
      <data key="d0">The University of Hong Kong</data>
      <data key="d1">organization</data>
      <data key="d2">The University of Hong Kong is an educational institution where the authors of RAG-Anything are affiliated.</data>
      <data key="d3">chunk-47439f39943c500ebbd82430fcd5a47f</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918074</data>
    </node>
    <node id="Knowledge Repositories">
      <data key="d0">Knowledge Repositories</data>
      <data key="d1">concept</data>
      <data key="d2">Knowledge repositories refer to collections of information that contain multimodal content, including text, visuals, and structured data.</data>
      <data key="d3">chunk-47439f39943c500ebbd82430fcd5a47f</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918075</data>
    </node>
    <node id="Multimodal Documents">
      <data key="d0">Multimodal Documents</data>
      <data key="d1">concept</data>
      <data key="d2">Multimodal documents are text documents that contain rich combinations of textual content, visual elements, and structured tables.</data>
      <data key="d3">chunk-47439f39943c500ebbd82430fcd5a47f</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918075</data>
    </node>
    <node id="Long Context Scenarios">
      <data key="d0">Long Context Scenarios</data>
      <data key="d1">concept</data>
      <data key="d2">Long context scenarios refer to situations in which relevant evidence is distributed across multiple modalities and sections of document content.</data>
      <data key="d3">chunk-47439f39943c500ebbd82430fcd5a47f</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918075</data>
    </node>
    <node id="Technical Challenges">
      <data key="d0">Technical Challenges</data>
      <data key="d1">concept</data>
      <data key="d2">Technical Challenges are difficulties associated with integrating multimodal content and ensuring accurate retrieval across modalities.</data>
      <data key="d3">chunk-47439f39943c500ebbd82430fcd5a47f</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918075</data>
    </node>
    <node id="Abootorabi et al.">
      <data key="d0">Abootorabi et al.</data>
      <data key="d1">person</data>
      <data key="d2">Abootorabi et al. are referenced as researchers discussing multimodal information and its integration in RAG frameworks.</data>
      <data key="d3">chunk-47439f39943c500ebbd82430fcd5a47f</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918075</data>
    </node>
    <node id="Large Language Models">
      <data key="d0">Large Language Models</data>
      <data key="d3">chunk-b05cce72ca9f7c1081afa465d0eb6116</data>
      <data key="d2">Zhang et al. provide insights into the limitations of large language models related to long-context inputs and multi-hop queries.</data>
      <data key="d1">UNKNOWN</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918078</data>
    </node>
    <node id="retrieval accuracy">
      <data key="d0">retrieval accuracy</data>
      <data key="d3">chunk-b05cce72ca9f7c1081afa465d0eb6116</data>
      <data key="d2">Bei et al. found that graph structures improve both retrieval efficiency and reasoning accuracy within document systems.</data>
      <data key="d1">UNKNOWN</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918078</data>
    </node>
    <node id="GNN-RAG">
      <data key="d0">GNN-RAG</data>
      <data key="d3">chunk-b05cce72ca9f7c1081afa465d0eb6116</data>
      <data key="d2">Mavromatis &amp; Karypis made contributions to neural models like GNN-RAG for enhanced document retrieval.</data>
      <data key="d1">UNKNOWN</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918079</data>
    </node>
    <node id="HippoRAG">
      <data key="d0">HippoRAG</data>
      <data key="d3">chunk-b05cce72ca9f7c1081afa465d0eb6116</data>
      <data key="d2">Jimenez Gutierrez et al. advanced retrieval performance through the development of HippoRAG as a memory-augmented variant.</data>
      <data key="d1">UNKNOWN</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918079</data>
    </node>
    <node id="RAPTOR">
      <data key="d0">RAPTOR</data>
      <data key="d3">chunk-b05cce72ca9f7c1081afa465d0eb6116</data>
      <data key="d2">Sarthi et al.'s RAPTOR method integrates information for enhancing multi-level reasoning in retrieval systems.</data>
      <data key="d1">UNKNOWN</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918079</data>
    </node>
    <node id="ArchRAG">
      <data key="d0">ArchRAG</data>
      <data key="d3">chunk-b05cce72ca9f7c1081afa465d0eb6116</data>
      <data key="d2">Wang et al. contributed to the knowledge aggregation approaches within the retrieval-augmented generation framework through ArchRAG.</data>
      <data key="d1">UNKNOWN</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918080</data>
    </node>
    <node id="multimodal RAG systems">
      <data key="d0">multimodal RAG systems</data>
      <data key="d3">chunk-b05cce72ca9f7c1081afa465d0eb6116</data>
      <data key="d2">Appendix A.5 offers an examination of challenges faced by current multimodal RAG systems through systematic failure case analysis.</data>
      <data key="d1">UNKNOWN</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918080</data>
    </node>
    <node id="structural information">
      <data key="d0">structural information</data>
      <data key="d3">chunk-b05cce72ca9f7c1081afa465d0eb6116</data>
      <data key="d2">MMGraphRAG fails to extract structural information effectively due to its treatment of tables and formulas as plain text.</data>
      <data key="d1">UNKNOWN</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918080</data>
    </node>
    <node id="visual understanding">
      <data key="d0">visual understanding</data>
      <data key="d3">chunk-b05cce72ca9f7c1081afa465d0eb6116</data>
      <data key="d2">VideoRAG relies on dual-channel architectures for visual understanding but may sacrifice vital information during text conversion.</data>
      <data key="d1">UNKNOWN</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918081</data>
    </node>
    <node id="document layouts">
      <data key="d0">document layouts</data>
      <data key="d3">chunk-b05cce72ca9f7c1081afa465d0eb6116</data>
      <data key="d2">VisRAG preserves document layouts as images but does not capture detailed relationships within the content accurately.</data>
      <data key="d1">UNKNOWN</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918081</data>
    </node>
    <node id="Rigid Spatial Processing Patterns">
      <data key="d0">Rigid Spatial Processing Patterns</data>
      <data key="d3">chunk-8a30975c53ad9871d2711abf552b151f</data>
      <data key="d2">Rigid spatial processing patterns are identified as limitations in multimodal RAG systems, failing to adapt to non-standard document layouts.</data>
      <data key="d1">UNKNOWN</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918086</data>
    </node>
    <node id="wages and salaries">
      <data key="d0">wages and salaries</data>
      <data key="d3">chunk-ea853774b3241b5f39f26edf475d4bd5</data>
      <data key="d2">Novo Nordisk's financial reporting includes metrics regarding their spending on wages and salaries.</data>
      <data key="d1">UNKNOWN</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918086</data>
    </node>
    <node id="Systematic Failure Case Examination">
      <data key="d0">Systematic Failure Case Examination</data>
      <data key="d3">chunk-8a30975c53ad9871d2711abf552b151f</data>
      <data key="d2">Systematic failure case examination is a method used to identify issues within multimodal RAG systems.</data>
      <data key="d1">UNKNOWN</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918088</data>
    </node>
    <node id="Decomposition of Multimodal Knowledge Sources (equation)">
      <data key="d0">Decomposition of Multimodal Knowledge Sources (equation)</data>
      <data key="d1">equation</data>
      <data key="d2">This equation captures the decomposition of a knowledge source into atomic content units, each defined by modality type and raw content, forming the basis for effective multimodal representation in the RAG-Anything framework.</data>
      <data key="d3">chunk-88ebe14996977f568b531a51ff2b02e2</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918157</data>
    </node>
    <node id="RAG-Anything Framework Overview (image)">
      <data key="d0">RAG-Anything Framework Overview (image)</data>
      <data key="d1">image</data>
      <data key="d2">This diagram provides a comprehensive overview of the RAG-Anything framework's architecture, focusing on the integration and retrieval of multimodal knowledge. It visualizes the process from input document parsing to cross-modal graph construction and query-based response generation. This visual guide aids in understanding how the system unifies and processes diverse data formats to enhance knowledge retrieval.</data>
      <data key="d3">chunk-92698bb9a611d4820a4578e37e74e670</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918157</data>
    </node>
    <node id="Dual-Graph Construction Mapping (equation)">
      <data key="d0">Dual-Graph Construction Mapping (equation)</data>
      <data key="d1">equation</data>
      <data key="d2">This equation defines the transformation of a contextual description of non-textual content into a structured graph representation, facilitating enhanced cross-modal retrieval in the RAG-Anything framework.</data>
      <data key="d3">chunk-2d23b106580d44e93ef9110173f0cfaf</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918157</data>
    </node>
    <node id="Multimodal Graph Construction Equation (equation)">
      <data key="d0">Multimodal Graph Construction Equation (equation)</data>
      <data key="d1">equation</data>
      <data key="d2">This equation constructs a unified multimodal knowledge graph by integrating multimodal entities and their intra-chunk relationships, enabling enhanced contextual understanding and retrieval capabilities in multimodal document processing.</data>
      <data key="d3">chunk-1b0413a81536bbb879f772d32b8c0dfe</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918157</data>
    </node>
    <node id="Unified Embedding Function for Multimodal Knowledge (equation)">
      <data key="d0">Unified Embedding Function for Multimodal Knowledge (equation)</data>
      <data key="d1">equation</data>
      <data key="d2">The equation captures the process of embedding various components in a knowledge graph, facilitating unified representation and retrieval in multimodal systems by transforming entities, relationships, and atomic chunks into a shared dense representation.</data>
      <data key="d3">chunk-11b9991810e9963c09e4017983a04dc3</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918157</data>
    </node>
    <node id="Statistics of Experimental Datasets (table)">
      <data key="d0">Statistics of Experimental Datasets (table)</data>
      <data key="d1">table</data>
      <data key="d2">This table lists statistics from DocBench and MMLongBench datasets, highlighting key metrics like document count and average tokens. It is integral to evaluating RAG-Anything's multimodal capabilities, as these datasets provide challenges reflective of real-world document complexities.</data>
      <data key="d3">chunk-3dd00be716f65d39ed28040e8449c044</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918157</data>
    </node>
    <node id="Multimodal Language Model Response Function (equation)">
      <data key="d0">Multimodal Language Model Response Function (equation)</data>
      <data key="d1">equation</data>
      <data key="d2">This equation defines how a multimodal language model computes a response based on a query and its associated textual and visual context, crucial for effective multimodal information synthesis.</data>
      <data key="d3">chunk-3fde5aaf8f821b4be0b48eb6bfb98337</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918157</data>
    </node>
    <node id="DocBench Multimodal Performance Table (table)">
      <data key="d0">DocBench Multimodal Performance Table (table)</data>
      <data key="d1">table</data>
      <data key="d2">This table compares accuracy across multimodal document processing methods in the DocBench dataset, highlighting RAG-Anything's superior performance, particularly in Multimodal contexts, demonstrating its effectiveness in addressing complex multimodal understanding challenges.</data>
      <data key="d3">chunk-c74c755aa200873b54b093c4aef3485e</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918157</data>
    </node>
    <node id="MMLongBench Performance Comparison Table (table)">
      <data key="d0">MMLongBench Performance Comparison Table (table)</data>
      <data key="d1">table</data>
      <data key="d2">The table compares the accuracy of RAG-Anything with other models (GPT-4o-mini, LightRAG, MMGraphRAG) on MMLongBench across various document domains. RAG-Anything outperforms the others, highlighting its superior multimodal integration abilities in handling complex, long-context documents.</data>
      <data key="d3">chunk-bed5ba3a860481b1c29ee139ebb94960</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918157</data>
    </node>
    <node id="Performance Evaluation Charts (image)">
      <data key="d0">Performance Evaluation Charts (image)</data>
      <data key="d1">image</data>
      <data key="d2">This image contains four charts that evaluate the performance of 'RAGAnything' and 'MMGraphRAG' on 'DocBench' and 'MMLongBench'. It highlights how 'RAGAnything' generally achieves higher accuracy than 'MMGraphRAG' across various document lengths, making it useful for analyzing performance metrics in research contexts.</data>
      <data key="d3">chunk-0ed9b2ed0a4bc6dedef04c5c1a224400</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918157</data>
    </node>
    <node id="Ablation Study Results on DocBench (table)">
      <data key="d0">Ablation Study Results on DocBench (table)</data>
      <data key="d1">table</data>
      <data key="d2">This table compares the performance of 'Chunk-only', 'w/o Reranker', and 'RAG-Anything' in various document domains, highlighting how RAG-Anything's dual-graph construction surpasses traditional methods by ensuring accurate multimodal processing, aligned with the discussion on the framework's architectural validation.</data>
      <data key="d3">chunk-260174f5a5690a7e51bdf14e210da8b4</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918157</data>
    </node>
    <node id="Multimodal Document with t-SNE Analysis (image)">
      <data key="d0">Multimodal Document with t-SNE Analysis (image)</data>
      <data key="d1">image</data>
      <data key="d2">This image illustrates a comparison in cluster separation within style spaces of DAE and VAE models using t-SNE plots, within the context of a multimodal document retrieval task indicated by a bolded query prompt.</data>
      <data key="d3">chunk-76f41ba1f43d29b4684e8f0113c3efae</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918157</data>
    </node>
    <node id="Novo Nordisk Financial Analysis Image (image)">
      <data key="d0">Novo Nordisk Financial Analysis Image (image)</data>
      <data key="d1">image</data>
      <data key="d2">The image illustrates how to identify evidence in multimodal documents, focusing on a financial table showing Novo Nordisk's wages and salaries for 2020. The composition visually explains the extraction of relevant data from complex document layouts.</data>
      <data key="d3">chunk-fb95b7d9ca41e3520dd06a9228985544</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918157</data>
    </node>
    <node id="Mathematical Equation">
      <data key="d0">Mathematical Equation</data>
      <data key="d1">concept</data>
      <data key="d2">The Mathematical Equation represents a process of decomposition, where a knowledge source is transformed into atomic content units.&lt;SEP&gt;The Mathematical Equation is a representation used in the analysis of multimodal vertex entities for knowledge graph construction.&lt;SEP&gt;The Mathematical Equation represents a function used for constructing a comprehensive embedding table pivotal in knowledge representation for multimodal retrieval systems.</data>
      <data key="d3">chunk-88ebe14996977f568b531a51ff2b02e2&lt;SEP&gt;chunk-1b0413a81536bbb879f772d32b8c0dfe&lt;SEP&gt;chunk-11b9991810e9963c09e4017983a04dc3</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918192</data>
    </node>
    <node id="Decompose">
      <data key="d0">Decompose</data>
      <data key="d1">method</data>
      <data key="d2">Decompose is the process described in the equation, transforming a knowledge source into individual content units characterized by modality type and raw content.</data>
      <data key="d3">chunk-88ebe14996977f568b531a51ff2b02e2</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918192</data>
    </node>
    <node id="Atomic Content Units">
      <data key="d0">Atomic Content Units</data>
      <data key="d1">concept</data>
      <data key="d2">Atomic Content Units are the resultant components derived from a knowledge source, each defined by its modality type and raw content.</data>
      <data key="d3">chunk-88ebe14996977f568b531a51ff2b02e2</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918192</data>
    </node>
    <node id="Knowledge Source">
      <data key="d0">Knowledge Source</data>
      <data key="d1">concept</data>
      <data key="d2">Knowledge Source refers to the original information contained within a multimodal corpus before decomposition into atomic units.</data>
      <data key="d3">chunk-88ebe14996977f568b531a51ff2b02e2</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918192</data>
    </node>
    <node id="RAG-Anything Framework">
      <data key="d0">RAG-Anything Framework</data>
      <data key="d1">concept</data>
      <data key="d2">RAG-Anything Framework is a system designed to structure and represent heterogeneous information for multimodal knowledge indexing and retrieval.&lt;SEP&gt;RAG-Anything Framework is a system that transforms heterogeneous knowledge into a unified graph structure.</data>
      <data key="d3">chunk-2d23b106580d44e93ef9110173f0cfaf&lt;SEP&gt;chunk-88ebe14996977f568b531a51ff2b02e2</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918192</data>
    </node>
    <node id="Multimodal Corpus">
      <data key="d0">Multimodal Corpus</data>
      <data key="d1">concept</data>
      <data key="d2">The Multimodal Corpus is a collection of varied types of information that serves as the source for knowledge decomposition into atomic content units.</data>
      <data key="d3">chunk-88ebe14996977f568b531a51ff2b02e2</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918192</data>
    </node>
    <node id="Modality Type">
      <data key="d0">Modality Type</data>
      <data key="d1">concept</data>
      <data key="d2">Modality Type refers to the classification of content units based on their nature, such as text, images, tables, or equations.</data>
      <data key="d3">chunk-88ebe14996977f568b531a51ff2b02e2</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918192</data>
    </node>
    <node id="Mathematical Equation Analysis">
      <data key="d0">Mathematical Equation Analysis</data>
      <data key="d1">concept</data>
      <data key="d2">Mathematical Equation Analysis focuses on analyzing equations and their implications within mathematical frameworks, particularly in relation to graph structures.</data>
      <data key="d3">chunk-2d23b106580d44e93ef9110173f0cfaf</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918192</data>
    </node>
    <node id="Equation">
      <data key="d0">Equation</data>
      <data key="d1">mathematicalnotation</data>
      <data key="d2">The equation ( \mathcal V _ { j } , \mathcal E _ { j } ) = R ( d _ { j } ^ { \mathrm { c h u n k } } ) is a representation of a mapping in mathematical analysis.</data>
      <data key="d3">chunk-2d23b106580d44e93ef9110173f0cfaf</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918193</data>
    </node>
    <node id="Graph Structure">
      <data key="d0">Graph Structure</data>
      <data key="d1">concept</data>
      <data key="d2">Graph Structure is a representation that illustrates the interrelations of entities in a visual manner, derived from mathematical equations.</data>
      <data key="d3">chunk-2d23b106580d44e93ef9110173f0cfaf</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918193</data>
    </node>
    <node id="Set of Vertices">
      <data key="d0">Set of Vertices</data>
      <data key="d1">concept</data>
      <data key="d2">Set of Vertices refers to the nodes in a graph representing atomic units of knowledge.</data>
      <data key="d3">chunk-2d23b106580d44e93ef9110173f0cfaf</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918193</data>
    </node>
    <node id="Set of Edges">
      <data key="d0">Set of Edges</data>
      <data key="d1">concept</data>
      <data key="d2">Set of Edges denotes the relationships between entities within a graph structure.</data>
      <data key="d3">chunk-2d23b106580d44e93ef9110173f0cfaf</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918193</data>
    </node>
    <node id="Generated Textual Description">
      <data key="d0">Generated Textual Description</data>
      <data key="d1">content</data>
      <data key="d2">Generated Textual Description encapsulates contextual semantics of a content unit, indexed under a specific methodology.</data>
      <data key="d3">chunk-2d23b106580d44e93ef9110173f0cfaf</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918193</data>
    </node>
    <node id="Dual-Graph Methodology">
      <data key="d0">Dual-Graph Methodology</data>
      <data key="d1">method</data>
      <data key="d2">Dual-Graph Methodology is a proposed framework for indexing and representing relationships in heterogeneous knowledge.</data>
      <data key="d3">chunk-2d23b106580d44e93ef9110173f0cfaf</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918193</data>
    </node>
    <node id="Cross-Modal Retrieval">
      <data key="d0">Cross-Modal Retrieval</data>
      <data key="d1">concept</data>
      <data key="d2">Cross-Modal Retrieval refers to the ability to retrieve information across different types of data modalities.</data>
      <data key="d3">chunk-2d23b106580d44e93ef9110173f0cfaf</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918193</data>
    </node>
    <node id="Structured Knowledge Representation">
      <data key="d0">Structured Knowledge Representation</data>
      <data key="d1">concept</data>
      <data key="d2">Structured Knowledge Representation focuses on organizing knowledge in a way that is beneficial for AI systems.</data>
      <data key="d3">chunk-2d23b106580d44e93ef9110173f0cfaf</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918193</data>
    </node>
    <node id="Mapping of Content Unit">
      <data key="d0">Mapping of Content Unit</data>
      <data key="d1">concept</data>
      <data key="d2">Mapping of Content Unit refers to the process of relating a detailed description of non-textual content to a graph structure.</data>
      <data key="d3">chunk-2d23b106580d44e93ef9110173f0cfaf</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918194</data>
    </node>
    <node id="Function R">
      <data key="d0">Function R</data>
      <data key="d1">method</data>
      <data key="d2">Function R is responsible for transforming generated textual descriptions into a structured format that reveals underlying relationships.</data>
      <data key="d3">chunk-2d23b106580d44e93ef9110173f0cfaf</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918194</data>
    </node>
    <node id="DocBench Accuracy">
      <data key="d0">DocBench Accuracy</data>
      <data key="d1">concept</data>
      <data key="d2">DocBench Accuracy is a metric represented in a line graph, showing the performance evaluation of 'RAGAnything' and 'MMGraphRAG' across different page ranges.</data>
      <data key="d3">chunk-0ed9b2ed0a4bc6dedef04c5c1a224400</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918194</data>
    </node>
    <node id="RAGAnything">
      <data key="d0">RAGAnything</data>
      <data key="d1">data</data>
      <data key="d2">RAGAnything is a data series in the 'DocBench Accuracy' chart, showing higher accuracy compared to 'MMGraphRAG'.&lt;SEP&gt;RAGAnything is a multimodal document processing method showcasing balanced performance and achieving the highest accuracy in the Multimodal category with 76.3%.</data>
      <data key="d3">chunk-c74c755aa200873b54b093c4aef3485e&lt;SEP&gt;chunk-0ed9b2ed0a4bc6dedef04c5c1a224400</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918194</data>
    </node>
    <node id="DocBench QA Counts">
      <data key="d0">DocBench QA Counts</data>
      <data key="d1">concept</data>
      <data key="d2">DocBench QA Counts is a bar chart representing the 'QA Pair Count' corresponding to different page ranges.</data>
      <data key="d3">chunk-0ed9b2ed0a4bc6dedef04c5c1a224400</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918194</data>
    </node>
    <node id="MMLongBench Accuracy">
      <data key="d0">MMLongBench Accuracy</data>
      <data key="d1">concept</data>
      <data key="d2">MMLongBench Accuracy is a metric displayed in a line graph similar to 'DocBench Accuracy', representing different accuracy metrics for benchmarks.</data>
      <data key="d3">chunk-0ed9b2ed0a4bc6dedef04c5c1a224400</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918194</data>
    </node>
    <node id="MMLongBench QA Counts">
      <data key="d0">MMLongBench QA Counts</data>
      <data key="d1">concept</data>
      <data key="d2">MMLongBench QA Counts is a bar chart showing 'QA Pair Count' similar to 'DocBench QA Counts'.</data>
      <data key="d3">chunk-0ed9b2ed0a4bc6dedef04c5c1a224400</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918194</data>
    </node>
    <node id="Four Separate Charts">
      <data key="d0">Four Separate Charts</data>
      <data key="d1">concept</data>
      <data key="d2">Four separate charts are displayed in a grid layout, providing various metrics about performance evaluation.</data>
      <data key="d3">chunk-0ed9b2ed0a4bc6dedef04c5c1a224400</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918194</data>
    </node>
    <node id="Page Range Categories">
      <data key="d0">Page Range Categories</data>
      <data key="d1">data</data>
      <data key="d2">Page range categories are defined as 1-10, 11-50, 51-100, 101-200, and 200+, representing the x-axis in the charts.</data>
      <data key="d3">chunk-0ed9b2ed0a4bc6dedef04c5c1a224400</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918194</data>
    </node>
    <node id="DocBench Dataset">
      <data key="d0">DocBench Dataset</data>
      <data key="d1">data</data>
      <data key="d2">The DocBench Dataset is a collection used for benchmarking the performance of multimodal document processing methods across various domains and types.</data>
      <data key="d3">chunk-c74c755aa200873b54b093c4aef3485e</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918194</data>
    </node>
    <node id="Academia">
      <data key="d0">Academia</data>
      <data key="d1">concept</data>
      <data key="d2">Academia is one of the domain categories referenced in the comparative analysis of multimodal document processing methods.</data>
      <data key="d3">chunk-c74c755aa200873b54b093c4aef3485e</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918195</data>
    </node>
    <node id="Finance">
      <data key="d0">Finance</data>
      <data key="d1">concept</data>
      <data key="d2">Finance is a domain category included in the analysis, where RAGAnything achieved a competitive accuracy of 67.0%.</data>
      <data key="d3">chunk-c74c755aa200873b54b093c4aef3485e</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918195</data>
    </node>
    <node id="Government">
      <data key="d0">Government</data>
      <data key="d1">concept</data>
      <data key="d2">Government is a domain category in the analysis, with methods like MMGraphRAG showing varying performance scores.</data>
      <data key="d3">chunk-c74c755aa200873b54b093c4aef3485e</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918195</data>
    </node>
    <node id="Law">
      <data key="d0">Law</data>
      <data key="d1">concept</data>
      <data key="d2">Law is a domain category mentioned in the comparative analysis of document processing methods.</data>
      <data key="d3">chunk-c74c755aa200873b54b093c4aef3485e</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918195</data>
    </node>
    <node id="News">
      <data key="d0">News</data>
      <data key="d1">concept</data>
      <data key="d2">News is a domain category in the analysis, with RAGAnything scoring 66.3% accuracy, highlighting its effectiveness in this area.</data>
      <data key="d3">chunk-c74c755aa200873b54b093c4aef3485e</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918195</data>
    </node>
    <node id="Text-only">
      <data key="d0">Text-only</data>
      <data key="d1">concept</data>
      <data key="d2">Text-only refers to a document type categorized in the analysis of multimodal methods.</data>
      <data key="d3">chunk-c74c755aa200873b54b093c4aef3485e</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918195</data>
    </node>
    <node id="Multimodal">
      <data key="d0">Multimodal</data>
      <data key="d1">concept</data>
      <data key="d2">Multimodal describes a type of document processing method that combines various forms of data input, as referenced in the analysis.</data>
      <data key="d3">chunk-c74c755aa200873b54b093c4aef3485e</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918195</data>
    </node>
    <node id="Unanswerable Queries">
      <data key="d0">Unanswerable Queries</data>
      <data key="d1">concept</data>
      <data key="d2">Unanswerable queries refer to a document type that challenges processing methods, where GPT-4o-mini scored only 43.8%.</data>
      <data key="d3">chunk-c74c755aa200873b54b093c4aef3485e</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918195</data>
    </node>
    <node id="Overall Accuracy">
      <data key="d0">Overall Accuracy</data>
      <data key="d1">concept</data>
      <data key="d2">Overall accuracy refers to the cumulative accuracy score achieved by the methods across all domains and document types in the analysis.</data>
      <data key="d3">chunk-c74c755aa200873b54b093c4aef3485e</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918195</data>
    </node>
    <node id="Dual-Graph Construction for Multimodal Knowledge">
      <data key="d0">Dual-Graph Construction for Multimodal Knowledge</data>
      <data key="d1">method</data>
      <data key="d2">Dual-Graph Construction for Multimodal Knowledge visualizes the processing of text and multimodal information into a knowledge graph.</data>
      <data key="d3">chunk-92698bb9a611d4820a4578e37e74e670</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918196</data>
    </node>
    <node id="Retrieval and Response Generation">
      <data key="d0">Retrieval and Response Generation</data>
      <data key="d1">method</data>
      <data key="d2">Retrieval and Response Generation describes the process of query input extraction and information retrieval culminating in a response.</data>
      <data key="d3">chunk-92698bb9a611d4820a4578e37e74e670</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918196</data>
    </node>
    <node id="VLM/LLM">
      <data key="d0">VLM/LLM</data>
      <data key="d1">method</data>
      <data key="d2">VLM/LLM (Visual Language Model/Large Language Model) processors are used to process text and multimodal information into a knowledge graph.</data>
      <data key="d3">chunk-92698bb9a611d4820a4578e37e74e670</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918196</data>
    </node>
    <node id="Vector Database">
      <data key="d0">Vector Database</data>
      <data key="d1">artifact</data>
      <data key="d2">The Vector Database includes layers for text and multimodal data processing categorized as 'Text VDB' and 'Multi-modal VDB'.</data>
      <data key="d3">chunk-92698bb9a611d4820a4578e37e74e670</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918196</data>
    </node>
    <node id="Query Processing">
      <data key="d0">Query Processing</data>
      <data key="d1">method</data>
      <data key="d2">Query Processing involves the steps necessary for extracting query input and generating responses.</data>
      <data key="d3">chunk-92698bb9a611d4820a4578e37e74e670</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918196</data>
    </node>
    <node id="Structured Content List">
      <data key="d0">Structured Content List</data>
      <data key="d1">artifact</data>
      <data key="d2">The Structured Content List compiles various input documents and includes processes such as hierarchical text extraction and metadata extraction.</data>
      <data key="d3">chunk-92698bb9a611d4820a4578e37e74e670</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918196</data>
    </node>
    <node id="Multi-Panel Figure Interpretation Case">
      <data key="d0">Multi-Panel Figure Interpretation Case</data>
      <data key="d1">concept</data>
      <data key="d2">The Multi-Panel Figure Interpretation Case is a visual analysis that requires understanding and distinguishing cluster separation patterns from style-space representations.</data>
      <data key="d3">chunk-76f41ba1f43d29b4684e8f0113c3efae</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918197</data>
    </node>
    <node id="t-SNE Plots">
      <data key="d0">t-SNE Plots</data>
      <data key="d1">method</data>
      <data key="d2">t-SNE plots are visualization techniques used to represent high-dimensional data, showcasing clusters in a style-space format.</data>
      <data key="d3">chunk-76f41ba1f43d29b4684e8f0113c3efae</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918197</data>
    </node>
    <node id="DAE Panel">
      <data key="d0">DAE Panel</data>
      <data key="d1">artifact</data>
      <data key="d2">The DAE Panel is a subpanel in the visual figure that displays a distinct separation of clusters, indicating clarity in style-space representation.</data>
      <data key="d3">chunk-76f41ba1f43d29b4684e8f0113c3efae</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918197</data>
    </node>
    <node id="VAE Panel">
      <data key="d0">VAE Panel</data>
      <data key="d1">artifact</data>
      <data key="d2">The VAE Panel is a subpanel in the visual figure that displays less distinct separation of clusters compared to the DAE Panel.</data>
      <data key="d3">chunk-76f41ba1f43d29b4684e8f0113c3efae</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918197</data>
    </node>
    <node id="Multimodal Document">
      <data key="d0">Multimodal Document</data>
      <data key="d1">concept</data>
      <data key="d2">The Multimodal Document is characterized by structured data and image snippets that provide context for the evidence presented in the analysis.</data>
      <data key="d3">chunk-76f41ba1f43d29b4684e8f0113c3efae</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918197</data>
    </node>
    <node id="Question Prompt">
      <data key="d0">Question Prompt</data>
      <data key="d1">content</data>
      <data key="d2">The Question Prompt is a part of the visual analysis that asks which model's style space shows clearer separation, specifically referenced in Figure 3.</data>
      <data key="d3">chunk-76f41ba1f43d29b4684e8f0113c3efae</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918197</data>
    </node>
    <node id="T">
      <data key="d0">T</data>
      <data key="d1">concept</data>
      <data key="d2">The variable T in the equation denotes the comprehensive embedding table being constructed for multimodal applications.</data>
      <data key="d3">chunk-11b9991810e9963c09e4017983a04dc3</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918197</data>
    </node>
    <node id="emb">
      <data key="d0">emb</data>
      <data key="d1">method</data>
      <data key="d2">The function emb(s) is an embedding function that generates a dense vector for each component, facilitating similarity searches in the embedding space.</data>
      <data key="d3">chunk-11b9991810e9963c09e4017983a04dc3</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918197</data>
    </node>
    <node id="V">
      <data key="d0">V</data>
      <data key="d1">data</data>
      <data key="d2">The set V represents graph entities in the context of the mathematical analysis.</data>
      <data key="d3">chunk-11b9991810e9963c09e4017983a04dc3</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918197</data>
    </node>
    <node id="E">
      <data key="d0">E</data>
      <data key="d1">data</data>
      <data key="d2">The set E refers to the relationships between the graph entities mentioned in the analysis.</data>
      <data key="d3">chunk-11b9991810e9963c09e4017983a04dc3</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918198</data>
    </node>
    <node id="c">
      <data key="d0">c</data>
      <data key="d1">data</data>
      <data key="d2">The variable c_{j_{j}} signifies atomic content chunks across varying modalities like text, images, and tables.</data>
      <data key="d3">chunk-11b9991810e9963c09e4017983a04dc3</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918198</data>
    </node>
    <node id="Multimodal Retrieval Systems">
      <data key="d0">Multimodal Retrieval Systems</data>
      <data key="d1">concept</data>
      <data key="d2">Multimodal Retrieval Systems are systems designed to retrieve information across various modalities, integrating text, images, and other forms of data.</data>
      <data key="d3">chunk-11b9991810e9963c09e4017983a04dc3</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918198</data>
    </node>
    <node id="Atom Content Chunks">
      <data key="d0">Atom Content Chunks</data>
      <data key="d1">concept</data>
      <data key="d2">Atom Content Chunks refer to distinct pieces of content across various modalities, including text, images, and equations, utilized in multimodal retrieval systems.</data>
      <data key="d3">chunk-11b9991810e9963c09e4017983a04dc3</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918198</data>
    </node>
    <node id="Similarity Searches">
      <data key="d0">Similarity Searches</data>
      <data key="d1">concept</data>
      <data key="d2">Similarity Searches are search techniques utilized to find and retrieve data based on the closeness of embeddings in the embedding space.</data>
      <data key="d3">chunk-11b9991810e9963c09e4017983a04dc3</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918198</data>
    </node>
    <node id="GPT-40-mini">
      <data key="d0">GPT-40-mini</data>
      <data key="d1">method</data>
      <data key="d2">GPT-40-mini is a model evaluated on MMLongBench, showing accuracy metrics against other models.</data>
      <data key="d3">chunk-bed5ba3a860481b1c29ee139ebb94960</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918198</data>
    </node>
    <node id="Research Reports/Introductions">
      <data key="d0">Research Reports/Introductions</data>
      <data key="d1">data</data>
      <data key="d2">Research Reports/Introductions represent one of the domain categories measured for accuracy in the MMLongBench evaluation.</data>
      <data key="d3">chunk-bed5ba3a860481b1c29ee139ebb94960</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918198</data>
    </node>
    <node id="Tutorials/Workshops">
      <data key="d0">Tutorials/Workshops</data>
      <data key="d1">data</data>
      <data key="d2">Tutorials/Workshops is a domain category evaluated for model performance on MMLongBench.</data>
      <data key="d3">chunk-bed5ba3a860481b1c29ee139ebb94960</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918198</data>
    </node>
    <node id="Academic Papers">
      <data key="d0">Academic Papers</data>
      <data key="d1">data</data>
      <data key="d2">Academic Papers is one of the domains evaluated in the MMLongBench accuracy measurement.</data>
      <data key="d3">chunk-bed5ba3a860481b1c29ee139ebb94960</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918198</data>
    </node>
    <node id="Guidebooks">
      <data key="d0">Guidebooks</data>
      <data key="d1">data</data>
      <data key="d2">Guidebooks are included as a domain category in the MMLongBench evaluation metrics.</data>
      <data key="d3">chunk-bed5ba3a860481b1c29ee139ebb94960</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918198</data>
    </node>
    <node id="Brochures">
      <data key="d0">Brochures</data>
      <data key="d1">data</data>
      <data key="d2">Brochures comprise a domain category assessed in the MMLongBench performance evaluation.</data>
      <data key="d3">chunk-bed5ba3a860481b1c29ee139ebb94960</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918199</data>
    </node>
    <node id="Administration/Industry Files">
      <data key="d0">Administration/Industry Files</data>
      <data key="d1">data</data>
      <data key="d2">Administration/Industry Files is a domain category evaluated within the MMLongBench framework.</data>
      <data key="d3">chunk-bed5ba3a860481b1c29ee139ebb94960</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918199</data>
    </node>
    <node id="Financial Reports">
      <data key="d0">Financial Reports</data>
      <data key="d1">data</data>
      <data key="d2">Financial Reports represent a significant domain category for measuring accuracy in MMLongBench evaluations.</data>
      <data key="d3">chunk-bed5ba3a860481b1c29ee139ebb94960</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918199</data>
    </node>
    <node id="Table 3">
      <data key="d0">Table 3</data>
      <data key="d1">artifact</data>
      <data key="d2">Table 3 is a representation of accuracy metrics on MMLongBench detailing model performance across various domains.</data>
      <data key="d3">chunk-bed5ba3a860481b1c29ee139ebb94960</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918199</data>
    </node>
    <node id="Accuracy">
      <data key="d0">Accuracy</data>
      <data key="d1">concept</data>
      <data key="d2">Accuracy refers to the measure of correctness of the models assessed in the MMLongBench evaluation.</data>
      <data key="d3">chunk-bed5ba3a860481b1c29ee139ebb94960</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918199</data>
    </node>
    <node id="Multimodel Document">
      <data key="d0">Multimodel Document</data>
      <data key="d1">content</data>
      <data key="d2">The Multimodel Document is a composite visual representation of a financial document, emphasizing specific areas using overlays and zoomed-in views.</data>
      <data key="d3">chunk-fb95b7d9ca41e3520dd06a9228985544</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918199</data>
    </node>
    <node id="Evidence Table">
      <data key="d0">Evidence Table</data>
      <data key="d1">data</data>
      <data key="d2">The Evidence Table features financial information about employee costs, including wages and salaries for the year 2020, along with comparative data for previous years.</data>
      <data key="d3">chunk-fb95b7d9ca41e3520dd06a9228985544</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918199</data>
    </node>
    <node id="Wages and Salaries">
      <data key="d0">Wages and Salaries</data>
      <data key="d1">data</data>
      <data key="d2">Wages and Salaries refer to the financial values presented in the Evidence Table, highlighting a specific amount for the year 2020.</data>
      <data key="d3">chunk-fb95b7d9ca41e3520dd06a9228985544</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918199</data>
    </node>
    <node id="26,778">
      <data key="d0">26,778</data>
      <data key="d1">data</data>
      <data key="d2">26,778 is the figure highlighted in the Evidence Table representing wages and salaries for the year 2020.</data>
      <data key="d3">chunk-fb95b7d9ca41e3520dd06a9228985544</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918199</data>
    </node>
    <node id="2020">
      <data key="d0">2020</data>
      <data key="d1">data</data>
      <data key="d2">2020 is the fiscal year for which the wages and salaries data is reported in the Evidence Table.</data>
      <data key="d3">chunk-fb95b7d9ca41e3520dd06a9228985544</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918199</data>
    </node>
    <node id="2019">
      <data key="d0">2019</data>
      <data key="d1">data</data>
      <data key="d2">2019 is one of the previous fiscal years included in the Evidence Table for comparative financial analysis.</data>
      <data key="d3">chunk-fb95b7d9ca41e3520dd06a9228985544</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918200</data>
    </node>
    <node id="2018">
      <data key="d0">2018</data>
      <data key="d1">data</data>
      <data key="d2">2018 is another previous fiscal year included in the Evidence Table for comparative financial analysis.</data>
      <data key="d3">chunk-fb95b7d9ca41e3520dd06a9228985544</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918200</data>
    </node>
    <node id="Financial Document">
      <data key="d0">Financial Document</data>
      <data key="d1">content</data>
      <data key="d2">The Financial Document contains detailed information regarding employee costs, as evidenced by the highlighted table section.</data>
      <data key="d3">chunk-fb95b7d9ca41e3520dd06a9228985544</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918200</data>
    </node>
    <node id="Table 1">
      <data key="d0">Table 1</data>
      <data key="d1">content</data>
      <data key="d2">Table 1 presents statistics of the experimental datasets DocBench and MMLongBench, including key metrics such as document counts and average tokens.</data>
      <data key="d3">chunk-3dd00be716f65d39ed28040e8449c044</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918200</data>
    </node>
    <node id="Statistics of Experimental Datasets">
      <data key="d0">Statistics of Experimental Datasets</data>
      <data key="d1">concept</data>
      <data key="d2">The Statistics of Experimental Datasets refers to the key metrics organized in Table 1, providing insights into the characteristics of DocBench and MMLongBench.</data>
      <data key="d3">chunk-3dd00be716f65d39ed28040e8449c044</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918200</data>
    </node>
    <node id="Ablation Study">
      <data key="d0">Ablation Study</data>
      <data key="d1">event</data>
      <data key="d2">An ablation study analyzing performance metrics of different methods in various domains and types.</data>
      <data key="d3">chunk-260174f5a5690a7e51bdf14e210da8b4</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918200</data>
    </node>
    <node id="Chunk-Only">
      <data key="d0">Chunk-Only</data>
      <data key="d1">method</data>
      <data key="d2">The "Chunk-only" method relies solely on traditional chunk-based retrieval, bypassing dual-graph construction.</data>
      <data key="d3">chunk-260174f5a5690a7e51bdf14e210da8b4</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918200</data>
    </node>
    <node id="W/O Reranker">
      <data key="d0">W/O Reranker</data>
      <data key="d1">method</data>
      <data key="d2">The "w/o Reranker" method eliminates cross-modal reranking but maintains the core graph-based architecture.</data>
      <data key="d3">chunk-260174f5a5690a7e51bdf14e210da8b4</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918200</data>
    </node>
    <node id="Performance Metrics">
      <data key="d0">Performance Metrics</data>
      <data key="d1">data</data>
      <data key="d2">Performance metrics provide data indicating the effectiveness of different methods across various domains and classifications.</data>
      <data key="d3">chunk-260174f5a5690a7e51bdf14e210da8b4</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918200</data>
    </node>
    <node id="Academic Domain">
      <data key="d0">Academic Domain</data>
      <data key="d1">concept</data>
      <data key="d2">The Academic domain measures the performance of retrieval methods in educational contexts.</data>
      <data key="d3">chunk-260174f5a5690a7e51bdf14e210da8b4</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918200</data>
    </node>
    <node id="Financial Domain">
      <data key="d0">Financial Domain</data>
      <data key="d1">concept</data>
      <data key="d2">The Financial domain measures the performance of retrieval methods in financial contexts.</data>
      <data key="d3">chunk-260174f5a5690a7e51bdf14e210da8b4</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918200</data>
    </node>
    <node id="Governmental Domain">
      <data key="d0">Governmental Domain</data>
      <data key="d1">concept</data>
      <data key="d2">The Governmental domain measures the performance of retrieval methods in governmental contexts.</data>
      <data key="d3">chunk-260174f5a5690a7e51bdf14e210da8b4</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918200</data>
    </node>
    <node id="Legal Domain">
      <data key="d0">Legal Domain</data>
      <data key="d1">concept</data>
      <data key="d2">The Legal domain measures the performance of retrieval methods in legal contexts.</data>
      <data key="d3">chunk-260174f5a5690a7e51bdf14e210da8b4</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918201</data>
    </node>
    <node id="News Domain">
      <data key="d0">News Domain</data>
      <data key="d1">concept</data>
      <data key="d2">The News domain measures the performance of retrieval methods in media contexts.</data>
      <data key="d3">chunk-260174f5a5690a7e51bdf14e210da8b4</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918201</data>
    </node>
    <node id="Text Type">
      <data key="d0">Text Type</data>
      <data key="d1">concept</data>
      <data key="d2">Text Type refers to the classification of the data being analyzed by retrieval methods.</data>
      <data key="d3">chunk-260174f5a5690a7e51bdf14e210da8b4</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918201</data>
    </node>
    <node id="Multimodal Type">
      <data key="d0">Multimodal Type</data>
      <data key="d1">concept</data>
      <data key="d2">Multimodal Type refers to data types that involve multiple modes of information, such as text and images.</data>
      <data key="d3">chunk-260174f5a5690a7e51bdf14e210da8b4</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918201</data>
    </node>
    <node id="Unannotated Type">
      <data key="d0">Unannotated Type</data>
      <data key="d1">concept</data>
      <data key="d2">Unannotated Type refers to raw data that has not been classified or labeled prior to analysis.</data>
      <data key="d3">chunk-260174f5a5690a7e51bdf14e210da8b4</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918201</data>
    </node>
    <node id="70.0% Overall Score">
      <data key="d0">70.0% Overall Score</data>
      <data key="d1">data</data>
      <data key="d2">The overall score of 70.0% indicates the performance level achieved by the methods evaluated in the ablation study.</data>
      <data key="d3">chunk-260174f5a5690a7e51bdf14e210da8b4</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918201</data>
    </node>
    <node id="62.4% Overall Score">
      <data key="d0">62.4% Overall Score</data>
      <data key="d1">data</data>
      <data key="d2">The "w/o Reranker" method achieved an overall score of 62.4% in the ablation study.</data>
      <data key="d3">chunk-260174f5a5690a7e51bdf14e210da8b4</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918201</data>
    </node>
    <node id="60.0% Overall Score">
      <data key="d0">60.0% Overall Score</data>
      <data key="d1">data</data>
      <data key="d2">The "Chunk-only" method achieved the lowest overall score of 60.0% in the ablation study.</data>
      <data key="d3">chunk-260174f5a5690a7e51bdf14e210da8b4</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918201</data>
    </node>
    <node id="Response">
      <data key="d0">Response</data>
      <data key="d1">concept</data>
      <data key="d2">Response is the output generated by a multimodal language model based on the provided query and associated context.</data>
      <data key="d3">chunk-3fde5aaf8f821b4be0b48eb6bfb98337</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918201</data>
    </node>
    <node id="VLM">
      <data key="d0">VLM</data>
      <data key="d1">concept</data>
      <data key="d2">VLM stands for multimodal language model, which processes both textual and visual information to generate responses to queries.</data>
      <data key="d3">chunk-3fde5aaf8f821b4be0b48eb6bfb98337</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918201</data>
    </node>
    <node id="q">
      <data key="d0">q</data>
      <data key="d1">data</data>
      <data key="d2">q represents the query input provided to the multimodal language model for analysis.</data>
      <data key="d3">chunk-3fde5aaf8f821b4be0b48eb6bfb98337</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918202</data>
    </node>
    <node id="P(q)">
      <data key="d0">P(q)</data>
      <data key="d1">data</data>
      <data key="d2">P(q) denotes the structured textual context created from retrieved multimodal chunks that accompany the query.</data>
      <data key="d3">chunk-3fde5aaf8f821b4be0b48eb6bfb98337</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918202</data>
    </node>
    <node id="V*(q)">
      <data key="d0">V*(q)</data>
      <data key="d1">data</data>
      <data key="d2">V*(q) refers to the dereferenced visual content associated with the query necessary for coherent synthesis.</data>
      <data key="d3">chunk-3fde5aaf8f821b4be0b48eb6bfb98337</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918202</data>
    </node>
    <node id="Hybrid Retrieval Mechanisms">
      <data key="d0">Hybrid Retrieval Mechanisms</data>
      <data key="d1">concept</data>
      <data key="d2">Hybrid retrieval mechanisms are strategies that enhance multimodal question-answering systems by bridging structural and semantic knowledge.</data>
      <data key="d3">chunk-3fde5aaf8f821b4be0b48eb6bfb98337</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918202</data>
    </node>
    <node id="Advanced AI Systems">
      <data key="d0">Advanced AI Systems</data>
      <data key="d1">concept</data>
      <data key="d2">Advanced AI systems are applications in areas like education that require comprehension of both textual and visual data to answer complex questions.</data>
      <data key="d3">chunk-3fde5aaf8f821b4be0b48eb6bfb98337</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918202</data>
    </node>
    <node id="Multimodal Question-Answering Systems">
      <data key="d0">Multimodal Question-Answering Systems</data>
      <data key="d1">concept</data>
      <data key="d2">Multimodal question-answering systems are advanced AI frameworks that utilize both textual and visual data to respond to queries effectively.</data>
      <data key="d3">chunk-3fde5aaf8f821b4be0b48eb6bfb98337</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918202</data>
    </node>
    <node id="Multimodal Vertex Entities">
      <data key="d0">Multimodal Vertex Entities</data>
      <data key="d1">concept</data>
      <data key="d2">Multimodal Vertex Entities refer to the nodes in the graph that serve as anchors for representing different types of data in knowledge graph construction.</data>
      <data key="d3">chunk-1b0413a81536bbb879f772d32b8c0dfe</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918202</data>
    </node>
    <node id="Graph Representation">
      <data key="d0">Graph Representation</data>
      <data key="d1">concept</data>
      <data key="d2">Graph Representation is a unified structure that preserves the relational context of diverse data types in a knowledge graph.</data>
      <data key="d3">chunk-1b0413a81536bbb879f772d32b8c0dfe</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918202</data>
    </node>
    <node id="Intra-Chunk Entities">
      <data key="d0">Intra-Chunk Entities</data>
      <data key="d1">concept</data>
      <data key="d2">Intra-Chunk Entities are individuals that enrich the graph structure by representing content within chunks.</data>
      <data key="d3">chunk-1b0413a81536bbb879f772d32b8c0dfe</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918202</data>
    </node>
    <node id="Belongs To Edges">
      <data key="d0">Belongs To Edges</data>
      <data key="d1">concept</data>
      <data key="d2">Belongs To Edges define the affiliations between different entities in a multimodal graph representation.</data>
      <data key="d3">chunk-1b0413a81536bbb879f772d32b8c0dfe</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918202</data>
    </node>
    <node id="Semantic Search">
      <data key="d0">Semantic Search</data>
      <data key="d1">concept</data>
      <data key="d2">Semantic Search refers to the application area where understanding complex relationships across different modalities is essential for better information retrieval.</data>
      <data key="d3">chunk-1b0413a81536bbb879f772d32b8c0dfe</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918203</data>
    </node>
    <node id="Natural Language Processing">
      <data key="d0">Natural Language Processing</data>
      <data key="d1">concept</data>
      <data key="d2">Natural Language Processing is a crucial application area that benefits from multimodal knowledge graph construction, improving comprehension of cross-modal relationships.</data>
      <data key="d3">chunk-1b0413a81536bbb879f772d32b8c0dfe</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918203</data>
    </node>
    <node id="Mathematical Analysis">
      <data key="d0">Mathematical Analysis</data>
      <data key="d1">concept</data>
      <data key="d2">Mathematical Analysis refers to the examination of equations and their implications in the context of multimodal knowledge graphs.</data>
      <data key="d3">chunk-1b0413a81536bbb879f772d32b8c0dfe</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918203</data>
    </node>
    <node id="63.4% Overall Score">
      <data key="d0">63.4% Overall Score</data>
      <data key="d3">chunk-260174f5a5690a7e51bdf14e210da8b4</data>
      <data key="d2">The "RAG-Anything" method is indicated to have an overall score of 63.4% in the evaluation.</data>
      <data key="d1">UNKNOWN</data>
      <data key="d4">research_paper.pdf</data>
      <data key="d5">1760918259</data>
    </node>
    <edge source="Synthesis Stage" target="Hybrid Retrieval Integration">
      <data key="d6">1.0</data>
      <data key="d7">Synthesis Stage integrates knowledge through Hybrid Retrieval Integration to produce coherent responses.</data>
      <data key="d8">knowledge integration,response generation</data>
      <data key="d9">chunk-50ef79f5820dd68dd3803e9eecf98e80</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918089</data>
    </edge>
    <edge source="Vision-Language Model" target="RAG-Anything">
      <data key="d6">1.0</data>
      <data key="d7">RAG-Anything employs a Vision-Language Model to integrate information from various sources for comprehensive document processing.</data>
      <data key="d8">integration,multimodal processing</data>
      <data key="d9">chunk-36eedc88c32f26a4209663bcf9de5e48</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918079</data>
    </edge>
    <edge source="DocBench" target="RAG-Anything">
      <data key="d6">4.0</data>
      <data key="d7">DocBench serves as a benchmark to comprehensively evaluate the performance of the RAG-Anything framework.&lt;SEP&gt;RAG-Anything is benchmarked against DocBench for multimodal Document Question Answering.&lt;SEP&gt;DocBench is used to evaluate the performance of the RAG-Anything model across various document lengths.&lt;SEP&gt;DocBench serves as a benchmark dataset for evaluating the multimodal understanding capabilities of RAG-Anything.</data>
      <data key="d8">benchmarking,dataset usage,evaluation,evaluation benchmark,experimentation,multimodal understanding,performance evaluation</data>
      <data key="d9">chunk-ea853774b3241b5f39f26edf475d4bd5&lt;SEP&gt;chunk-36eedc88c32f26a4209663bcf9de5e48&lt;SEP&gt;chunk-3dd00be716f65d39ed28040e8449c044&lt;SEP&gt;chunk-2342fe853447858443171ce27b6bf554</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918251</data>
    </edge>
    <edge source="DocBench" target="Table 1">
      <data key="d6">1.0</data>
      <data key="d7">Table 1 includes detailed statistics of the dataset DocBench, showing its characteristics and metrics.</data>
      <data key="d8">dataset overview,statistics</data>
      <data key="d9">chunk-3dd00be716f65d39ed28040e8449c044</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918250</data>
    </edge>
    <edge source="DocBench" target="Statistics of Experimental Datasets (table)">
      <data key="d6">10.0</data>
      <data key="d7">Entity DocBench belongs to Statistics of Experimental Datasets (table)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-3dd00be716f65d39ed28040e8449c044</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918253</data>
    </edge>
    <edge source="DocBench" target="Ablation Study">
      <data key="d6">1.0</data>
      <data key="d7">The Ablation Study utilizes DocBench to evaluate the effectiveness of different retrieval methods.</data>
      <data key="d8">evaluation benchmark,methodology assessment</data>
      <data key="d9">chunk-260174f5a5690a7e51bdf14e210da8b4</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918257</data>
    </edge>
    <edge source="DocBench" target="Ablation Study Results on DocBench (table)">
      <data key="d6">10.0</data>
      <data key="d7">Entity DocBench belongs to Ablation Study Results on DocBench (table)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-260174f5a5690a7e51bdf14e210da8b4</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918269</data>
    </edge>
    <edge source="MMLongBench" target="RAG-Anything">
      <data key="d6">5.0</data>
      <data key="d7">MMLongBench acts as a second benchmark used to validate the performance of RAG-Anything in multimodal retrieval scenarios.&lt;SEP&gt;RAG-Anything is evaluated on MMLongBench specifically focusing on long-context document comprehension.&lt;SEP&gt;MMLongBench is also used to analyze RAG-Anything's performance on document retrieval tasks.&lt;SEP&gt;MMLongBench provides another benchmark for testing RAG-Anything's performance in long-context comprehension.&lt;SEP&gt;RAG-Anything is evaluated on MMLongBench to assess its performance across various document understanding tasks.</data>
      <data key="d8">benchmarking,dataset usage,evaluation,evaluation benchmark,experimentation,model evaluation,multimodal understanding,performance evaluation,performance metric</data>
      <data key="d9">chunk-36eedc88c32f26a4209663bcf9de5e48&lt;SEP&gt;chunk-bed5ba3a860481b1c29ee139ebb94960&lt;SEP&gt;chunk-ea853774b3241b5f39f26edf475d4bd5&lt;SEP&gt;chunk-3dd00be716f65d39ed28040e8449c044&lt;SEP&gt;chunk-2342fe853447858443171ce27b6bf554</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918237</data>
    </edge>
    <edge source="MMLongBench" target="GPT-40-mini">
      <data key="d6">1.0</data>
      <data key="d7">GPT-40-mini is assessed on MMLongBench, showing comparative accuracy metrics against other models.</data>
      <data key="d8">model evaluation,performance metric</data>
      <data key="d9">chunk-bed5ba3a860481b1c29ee139ebb94960</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918238</data>
    </edge>
    <edge source="MMLongBench" target="LightRAG">
      <data key="d6">1.0</data>
      <data key="d7">LightRAG is evaluated on MMLongBench, providing specific accuracy metrics in various domains.</data>
      <data key="d8">model evaluation,performance metric</data>
      <data key="d9">chunk-bed5ba3a860481b1c29ee139ebb94960</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918239</data>
    </edge>
    <edge source="MMLongBench" target="MMGraphRAG">
      <data key="d6">1.0</data>
      <data key="d7">MMGraphRAG is also evaluated on MMLongBench, showing its performance across the defined document categories.</data>
      <data key="d8">model evaluation,performance metric</data>
      <data key="d9">chunk-bed5ba3a860481b1c29ee139ebb94960</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918240</data>
    </edge>
    <edge source="MMLongBench" target="Table 3">
      <data key="d6">1.0</data>
      <data key="d7">Table 3 provides a data representation of the models' accuracy on MMLongBench, demonstrating evaluations across multiple categories.</data>
      <data key="d8">data representation,model evaluation</data>
      <data key="d9">chunk-bed5ba3a860481b1c29ee139ebb94960</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918241</data>
    </edge>
    <edge source="MMLongBench" target="Accuracy">
      <data key="d6">1.0</data>
      <data key="d7">Accuracy serves as an evaluation metric in the MMLongBench framework to assess model performance.</data>
      <data key="d8">evaluation metric,performance measurement</data>
      <data key="d9">chunk-bed5ba3a860481b1c29ee139ebb94960</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918242</data>
    </edge>
    <edge source="MMLongBench" target="MMLongBench Performance Comparison Table (table)">
      <data key="d6">10.0</data>
      <data key="d7">Entity MMLongBench belongs to MMLongBench Performance Comparison Table (table)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-bed5ba3a860481b1c29ee139ebb94960</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918246</data>
    </edge>
    <edge source="MMLongBench" target="Table 1">
      <data key="d6">1.0</data>
      <data key="d7">Table 1 also includes statistics for MMLongBench, highlighting its unique aspects among the datasets.</data>
      <data key="d8">dataset overview,statistics</data>
      <data key="d9">chunk-3dd00be716f65d39ed28040e8449c044</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918251</data>
    </edge>
    <edge source="MMLongBench" target="Statistics of Experimental Datasets (table)">
      <data key="d6">10.0</data>
      <data key="d7">Entity MMLongBench belongs to Statistics of Experimental Datasets (table)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-3dd00be716f65d39ed28040e8449c044</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918252</data>
    </edge>
    <edge source="GPT-4o-mini" target="RAG-Anything">
      <data key="d6">1.0</data>
      <data key="d7">GPT-4o-mini is used as a baseline for evaluating the performance of RAG-Anything.</data>
      <data key="d8">baseline comparison,performance evaluation</data>
      <data key="d9">chunk-36eedc88c32f26a4209663bcf9de5e48</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918077</data>
    </edge>
    <edge source="GPT-4o-mini" target="DocBench Dataset">
      <data key="d6">1.0</data>
      <data key="d7">GPT-4o-mini's results are evaluated based on accuracy scores derived from the DocBench Dataset.</data>
      <data key="d8">evaluation,performance analysis</data>
      <data key="d9">chunk-c74c755aa200873b54b093c4aef3485e</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918217</data>
    </edge>
    <edge source="GPT-4o-mini" target="DocBench Multimodal Performance Table (table)">
      <data key="d6">10.0</data>
      <data key="d7">Entity GPT-4o-mini belongs to DocBench Multimodal Performance Table (table)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-c74c755aa200873b54b093c4aef3485e</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918219</data>
    </edge>
    <edge source="LightRAG" target="RAG-Anything">
      <data key="d6">1.0</data>
      <data key="d7">LightRAG is compared with RAG-Anything to assess differences in multimodal processing capabilities.</data>
      <data key="d8">method comparison,performance evaluation</data>
      <data key="d9">chunk-36eedc88c32f26a4209663bcf9de5e48</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918078</data>
    </edge>
    <edge source="LightRAG" target="Guo et al.">
      <data key="d6">1.0</data>
      <data key="d7">Guo et al. focused on optimizing retrieval structures through the development of LightRAG.</data>
      <data key="d8">optimization,research focus</data>
      <data key="d9">chunk-b05cce72ca9f7c1081afa465d0eb6116</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918079</data>
    </edge>
    <edge source="LightRAG" target="DocBench Dataset">
      <data key="d6">1.0</data>
      <data key="d7">LightRAG's performance metrics are assessed against the benchmarks set by the DocBench Dataset.</data>
      <data key="d8">evaluation,performance analysis</data>
      <data key="d9">chunk-c74c755aa200873b54b093c4aef3485e</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918218</data>
    </edge>
    <edge source="LightRAG" target="Text-only">
      <data key="d6">1.0</data>
      <data key="d7">LightRAG scored 85.0% in the Text-only category, indicating strong performance in handling text documents.</data>
      <data key="d8">accuracy score,document type performance</data>
      <data key="d9">chunk-c74c755aa200873b54b093c4aef3485e</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918219</data>
    </edge>
    <edge source="LightRAG" target="DocBench Multimodal Performance Table (table)">
      <data key="d6">10.0</data>
      <data key="d7">Entity LightRAG belongs to DocBench Multimodal Performance Table (table)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-c74c755aa200873b54b093c4aef3485e</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918220</data>
    </edge>
    <edge source="LightRAG" target="MMLongBench Performance Comparison Table (table)">
      <data key="d6">10.0</data>
      <data key="d7">Entity LightRAG belongs to MMLongBench Performance Comparison Table (table)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-bed5ba3a860481b1c29ee139ebb94960</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918240</data>
    </edge>
    <edge source="MMGraphRAG" target="RAG-Anything">
      <data key="d6">2.0</data>
      <data key="d7">MMGraphRAG's performance is compared with RAG-Anything to evaluate multimodal understanding improvements.&lt;SEP&gt;RAG-Anything and MMGraphRAG are compared for their performance on document retrieval tasks.</data>
      <data key="d8">document retrieval,method comparison,performance comparison,performance evaluation</data>
      <data key="d9">chunk-ea853774b3241b5f39f26edf475d4bd5&lt;SEP&gt;chunk-36eedc88c32f26a4209663bcf9de5e48</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918079</data>
    </edge>
    <edge source="MMGraphRAG" target="structural information">
      <data key="d6">1.0</data>
      <data key="d7">MMGraphRAG fails to extract structural information effectively due to its treatment of tables and formulas as plain text.</data>
      <data key="d8">method limitation,structural blindness</data>
      <data key="d9">chunk-b05cce72ca9f7c1081afa465d0eb6116</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918080</data>
    </edge>
    <edge source="MMGraphRAG" target="DocBench Accuracy">
      <data key="d6">1.0</data>
      <data key="d7">MMGraphRAG shows inferior performance in the DocBench Accuracy chart compared to RAGAnything.</data>
      <data key="d8">data comparison,performance metric</data>
      <data key="d9">chunk-0ed9b2ed0a4bc6dedef04c5c1a224400</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918213</data>
    </edge>
    <edge source="MMGraphRAG" target="Performance Evaluation Charts (image)">
      <data key="d6">10.0</data>
      <data key="d7">Entity MMGraphRAG belongs to Performance Evaluation Charts (image)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-0ed9b2ed0a4bc6dedef04c5c1a224400</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918215</data>
    </edge>
    <edge source="MMGraphRAG" target="Government">
      <data key="d6">1.0</data>
      <data key="d7">MMGraphRAG's evaluation indicates varying performance in the Government domain amongst the methods assessed.</data>
      <data key="d8">domain performance,evaluation</data>
      <data key="d9">chunk-c74c755aa200873b54b093c4aef3485e</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918218</data>
    </edge>
    <edge source="MMGraphRAG" target="DocBench Dataset">
      <data key="d6">1.0</data>
      <data key="d7">The performance of MMGraphRAG is reviewed using the accuracy percentages provided in the DocBench Dataset.</data>
      <data key="d8">evaluation,performance analysis</data>
      <data key="d9">chunk-c74c755aa200873b54b093c4aef3485e</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918219</data>
    </edge>
    <edge source="MMGraphRAG" target="DocBench Multimodal Performance Table (table)">
      <data key="d6">10.0</data>
      <data key="d7">Entity MMGraphRAG belongs to DocBench Multimodal Performance Table (table)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-c74c755aa200873b54b093c4aef3485e</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918220</data>
    </edge>
    <edge source="MMGraphRAG" target="MMLongBench Performance Comparison Table (table)">
      <data key="d6">10.0</data>
      <data key="d7">Entity MMGraphRAG belongs to MMLongBench Performance Comparison Table (table)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-bed5ba3a860481b1c29ee139ebb94960</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918243</data>
    </edge>
    <edge source="RAG-Anything" target="Financial Table Navigation">
      <data key="d6">1.0</data>
      <data key="d7">RAG-Anything is applied in Financial Table Navigation to improve the extraction of specific metrics from financial document analysis.</data>
      <data key="d8">document analysis,system application</data>
      <data key="d9">chunk-b05cce72ca9f7c1081afa465d0eb6116</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918080</data>
    </edge>
    <edge source="RAG-Anything" target="Key Insights">
      <data key="d6">1.0</data>
      <data key="d7">The implementation of RAG-Anything leads to key insights regarding its structure-aware design in enhancing document navigation and comprehension.</data>
      <data key="d8">framework benefits,operational advantages</data>
      <data key="d9">chunk-b05cce72ca9f7c1081afa465d0eb6116</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918081</data>
    </edge>
    <edge source="RAG-Anything" target="Appendix A.2">
      <data key="d6">1.0</data>
      <data key="d7">Appendix A.2 provides additional cases relevant to the applications and capabilities of RAG-Anything.</data>
      <data key="d8">additional cases,application insights</data>
      <data key="d9">chunk-b05cce72ca9f7c1081afa465d0eb6116</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918082</data>
    </edge>
    <edge source="RAG-Anything" target="Cross-Modal Knowledge Graph">
      <data key="d6">1.0</data>
      <data key="d7">RAG-Anything is responsible for constructing the Cross-Modal Knowledge Graph as part of its strategy for knowledge unification.</data>
      <data key="d8">multimodal representation,system design</data>
      <data key="d9">chunk-fb3b2a9642d9402ce461122c98a81fd6</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918082</data>
    </edge>
    <edge source="RAG-Anything" target="Text-Based Knowledge Graph">
      <data key="d6">1.0</data>
      <data key="d7">The Text-Based Knowledge Graph is built using methodologies applied by RAG-Anything to capture text semantics.</data>
      <data key="d8">construction methodology,text semantics</data>
      <data key="d9">chunk-fb3b2a9642d9402ce461122c98a81fd6</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918083</data>
    </edge>
    <edge source="RAG-Anything" target="Multimodal Large Language Models">
      <data key="d6">1.0</data>
      <data key="d7">RAG-Anything leverages Multimodal Large Language Models to enhance knowledge extraction and representation.</data>
      <data key="d8">model application,system functionality</data>
      <data key="d9">chunk-fb3b2a9642d9402ce461122c98a81fd6</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918084</data>
    </edge>
    <edge source="RAG-Anything" target="Local Neighborhood">
      <data key="d6">1.0</data>
      <data key="d7">The Local Neighborhood plays a critical role in RAG-Anything's context-aware processing for generating accurate entity representations.</data>
      <data key="d8">context-aware processing,entity representation</data>
      <data key="d9">chunk-fb3b2a9642d9402ce461122c98a81fd6</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918085</data>
    </edge>
    <edge source="RAG-Anything" target="Cross-Modal Hybrid Retrieval">
      <data key="d6">2.0</data>
      <data key="d7">RAG-Anything employs Cross-Modal Hybrid Retrieval to enhance the integration of diverse data types for better knowledge retrieval.&lt;SEP&gt;Cross-Modal Hybrid Retrieval is a key method employed by RAG-Anything to enhance knowledge retrieval.</data>
      <data key="d8">framework,framework capability,methodological approach,methodology</data>
      <data key="d9">chunk-47439f39943c500ebbd82430fcd5a47f&lt;SEP&gt;chunk-2342fe853447858443171ce27b6bf554</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918085</data>
    </edge>
    <edge source="RAG-Anything" target="Multimodal Knowledge Unification">
      <data key="d6">2.0</data>
      <data key="d7">Multimodal Knowledge Unification is a core principle that enhances the RAG-Anything framework by ensuring structured content is processed effectively.&lt;SEP&gt;RAG-Anything includes Multimodal Knowledge Unification as a component for integrating diverse document formats.</data>
      <data key="d8">framework component,framework enhancement,knowledge integration,knowledge representation</data>
      <data key="d9">chunk-92698bb9a611d4820a4578e37e74e670&lt;SEP&gt;chunk-2342fe853447858443171ce27b6bf554</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918224</data>
    </edge>
    <edge source="RAG-Anything" target="Multimodal Reality">
      <data key="d6">1.0</data>
      <data key="d7">RAG-Anything is designed to address the challenges posed by Multimodal Reality, recognizing the need for heterogeneous knowledge integration.</data>
      <data key="d8">alignment,system characteristic</data>
      <data key="d9">chunk-2342fe853447858443171ce27b6bf554</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918087</data>
    </edge>
    <edge source="RAG-Anything" target="Graph Construction">
      <data key="d6">1.0</data>
      <data key="d7">Graph construction is essential for the effectiveness of the RAG-Anything model in capturing complex relationships.</data>
      <data key="d8">architectural component,model effectiveness</data>
      <data key="d9">chunk-ea853774b3241b5f39f26edf475d4bd5</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918088</data>
    </edge>
    <edge source="RAG-Anything" target="Ablation Studies">
      <data key="d6">1.0</data>
      <data key="d7">Ablation studies are conducted to validate the architectural design and contributions of RAG-Anything.</data>
      <data key="d8">architectural evaluation,model validation</data>
      <data key="d9">chunk-ea853774b3241b5f39f26edf475d4bd5</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918089</data>
    </edge>
    <edge source="RAG-Anything" target="Figure 2">
      <data key="d6">1.0</data>
      <data key="d7">Figure 2 is referenced in the context of demonstrating the performance of the RAG-Anything model across styles.</data>
      <data key="d8">model performance,visualization reference</data>
      <data key="d9">chunk-ea853774b3241b5f39f26edf475d4bd5</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918089</data>
    </edge>
    <edge source="RAG-Anything" target="Figure 3">
      <data key="d6">1.0</data>
      <data key="d7">Figure 3 is referenced as a case study in examining RAG-Anythings ability to interpret multi-panel figures.</data>
      <data key="d8">case study,visualization reference</data>
      <data key="d9">chunk-ea853774b3241b5f39f26edf475d4bd5</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918090</data>
    </edge>
    <edge source="RAG-Anything" target="Figure 4">
      <data key="d6">1.0</data>
      <data key="d7">Figure 4 is used to demonstrate the RAG-Anything model's effectiveness in navigating financial tables.</data>
      <data key="d8">financial analysis,visualization reference</data>
      <data key="d9">chunk-ea853774b3241b5f39f26edf475d4bd5</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918091</data>
    </edge>
    <edge source="RAG-Anything" target="Zirui Guo">
      <data key="d6">1.0</data>
      <data key="d7">Zirui Guo contributed to the development of RAG-Anything as an author and researcher.</data>
      <data key="d8">author contribution,framework development</data>
      <data key="d9">chunk-47439f39943c500ebbd82430fcd5a47f</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918092</data>
    </edge>
    <edge source="RAG-Anything" target="Xubin Ren">
      <data key="d6">1.0</data>
      <data key="d7">Xubin Ren contributed to the development of RAG-Anything as an author and researcher.</data>
      <data key="d8">author contribution,framework development</data>
      <data key="d9">chunk-47439f39943c500ebbd82430fcd5a47f</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918092</data>
    </edge>
    <edge source="RAG-Anything" target="Lingrui Xu">
      <data key="d6">1.0</data>
      <data key="d7">Lingrui Xu contributed to the development of RAG-Anything as an author and researcher.</data>
      <data key="d8">author contribution,framework development</data>
      <data key="d9">chunk-47439f39943c500ebbd82430fcd5a47f</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918093</data>
    </edge>
    <edge source="RAG-Anything" target="Jiahao Zhang">
      <data key="d6">1.0</data>
      <data key="d7">Jiahao Zhang contributed to the development of RAG-Anything as an author and researcher.</data>
      <data key="d8">author contribution,framework development</data>
      <data key="d9">chunk-47439f39943c500ebbd82430fcd5a47f</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918095</data>
    </edge>
    <edge source="RAG-Anything" target="Chao Huang">
      <data key="d6">1.0</data>
      <data key="d7">Chao Huang contributed to the development of RAG-Anything as an author and researcher.</data>
      <data key="d8">author contribution,framework development</data>
      <data key="d9">chunk-47439f39943c500ebbd82430fcd5a47f</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918096</data>
    </edge>
    <edge source="RAG-Anything" target="The University of Hong Kong">
      <data key="d6">1.0</data>
      <data key="d7">RAG-Anything was developed by researchers affiliated with The University of Hong Kong.</data>
      <data key="d8">institutional affiliation,research development</data>
      <data key="d9">chunk-47439f39943c500ebbd82430fcd5a47f</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918097</data>
    </edge>
    <edge source="RAG-Anything" target="Retrieval-Augmented Generation (RAG)">
      <data key="d6">1.0</data>
      <data key="d7">RAG-Anything builds upon the principles of Retrieval-Augmented Generation (RAG) for multimodal capabilities.</data>
      <data key="d8">conceptual foundation,framework enhancement</data>
      <data key="d9">chunk-47439f39943c500ebbd82430fcd5a47f</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918097</data>
    </edge>
    <edge source="RAG-Anything" target="Knowledge Repositories">
      <data key="d6">1.0</data>
      <data key="d7">RAG-Anything aims to improve access to knowledge repositories that contain multimodal information.</data>
      <data key="d8">framework application,information sources</data>
      <data key="d9">chunk-47439f39943c500ebbd82430fcd5a47f</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918098</data>
    </edge>
    <edge source="RAG-Anything" target="Multimodal Documents">
      <data key="d6">1.0</data>
      <data key="d7">RAG-Anything specifically addresses the challenges presented by multimodal documents.</data>
      <data key="d8">document type,framework focus</data>
      <data key="d9">chunk-47439f39943c500ebbd82430fcd5a47f</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918099</data>
    </edge>
    <edge source="RAG-Anything" target="Long Context Scenarios">
      <data key="d6">1.0</data>
      <data key="d7">RAG-Anything is particularly designed to address challenges in long context scenarios.</data>
      <data key="d8">application context,framework necessity</data>
      <data key="d9">chunk-47439f39943c500ebbd82430fcd5a47f</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918099</data>
    </edge>
    <edge source="RAG-Anything" target="Technical Challenges">
      <data key="d6">1.0</data>
      <data key="d7">Technical Challenges are addressed by RAG-Anything to improve multimodal processing.</data>
      <data key="d8">development obstacle,solution requirement</data>
      <data key="d9">chunk-47439f39943c500ebbd82430fcd5a47f</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918100</data>
    </edge>
    <edge source="RAG-Anything" target="Abootorabi et al.">
      <data key="d6">1.0</data>
      <data key="d7">Abootorabi et al. discuss the significance of RAG-Anything in the context of multimodal knowledge representation.</data>
      <data key="d8">multimodal framework,research discussion</data>
      <data key="d9">chunk-47439f39943c500ebbd82430fcd5a47f</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918101</data>
    </edge>
    <edge source="RAG-Anything" target="Dual-Graph Construction for Multimodal Knowledge">
      <data key="d6">1.0</data>
      <data key="d7">RAG-Anything utilizes Dual-Graph Construction for visualizing the processing of multimodal information into a knowledge graph.</data>
      <data key="d8">framework component,knowledge graph</data>
      <data key="d9">chunk-92698bb9a611d4820a4578e37e74e670</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918225</data>
    </edge>
    <edge source="RAG-Anything" target="RAG-Anything Framework Overview (image)">
      <data key="d6">10.0</data>
      <data key="d7">Entity RAG-Anything belongs to RAG-Anything Framework Overview (image)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-92698bb9a611d4820a4578e37e74e670</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918227</data>
    </edge>
    <edge source="RAG-Anything" target="Research Reports/Introductions">
      <data key="d6">1.0</data>
      <data key="d7">RAG-Anything demonstrates superior performance in the Research Reports/Introductions category in MMLongBench.</data>
      <data key="d8">domain specific,superior performance</data>
      <data key="d9">chunk-bed5ba3a860481b1c29ee139ebb94960</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918238</data>
    </edge>
    <edge source="RAG-Anything" target="Financial Reports">
      <data key="d6">1.0</data>
      <data key="d7">RAG-Anything also shows superior performance in the Financial Reports category within the MMLongBench framework.</data>
      <data key="d8">domain specific,superior performance</data>
      <data key="d9">chunk-bed5ba3a860481b1c29ee139ebb94960</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918239</data>
    </edge>
    <edge source="RAG-Anything" target="MMLongBench Performance Comparison Table (table)">
      <data key="d6">10.0</data>
      <data key="d7">Entity RAG-Anything belongs to MMLongBench Performance Comparison Table (table)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-bed5ba3a860481b1c29ee139ebb94960</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918241</data>
    </edge>
    <edge source="RAG-Anything" target="Statistics of Experimental Datasets">
      <data key="d6">1.0</data>
      <data key="d7">The Statistics of Experimental Datasets inform the evaluation context for RAG-Anything, highlighting its importance in assessing multimodal document understanding.</data>
      <data key="d8">evaluation context,multimodal understanding</data>
      <data key="d9">chunk-3dd00be716f65d39ed28040e8449c044</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918252</data>
    </edge>
    <edge source="RAG-Anything" target="Statistics of Experimental Datasets (table)">
      <data key="d6">10.0</data>
      <data key="d7">Entity RAG-Anything belongs to Statistics of Experimental Datasets (table)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-3dd00be716f65d39ed28040e8449c044</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918255</data>
    </edge>
    <edge source="RAG-Anything" target="Performance Metrics">
      <data key="d6">1.0</data>
      <data key="d7">RAG-Anything's performance metrics demonstrate its effectiveness in various domains.</data>
      <data key="d8">effectiveness,performance evaluation</data>
      <data key="d9">chunk-260174f5a5690a7e51bdf14e210da8b4</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918256</data>
    </edge>
    <edge source="RAG-Anything" target="Ablation Study">
      <data key="d6">1.0</data>
      <data key="d7">The ablation study compares the performance of RAG-Anything against the other methods.</data>
      <data key="d8">method comparison,performance analysis</data>
      <data key="d9">chunk-260174f5a5690a7e51bdf14e210da8b4</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918256</data>
    </edge>
    <edge source="RAG-Anything" target="63.4% Overall Score">
      <data key="d6">1.0</data>
      <data key="d7">The "RAG-Anything" method is indicated to have an overall score of 63.4% in the evaluation.</data>
      <data key="d8">evaluation results,performance outcome</data>
      <data key="d9">chunk-260174f5a5690a7e51bdf14e210da8b4</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918259</data>
    </edge>
    <edge source="RAG-Anything" target="Ablation Study Results on DocBench (table)">
      <data key="d6">10.0</data>
      <data key="d7">Entity RAG-Anything belongs to Ablation Study Results on DocBench (table)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-260174f5a5690a7e51bdf14e210da8b4</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918261</data>
    </edge>
    <edge source="Carbon-Fiber Spikes" target="MMLongBench Performance Comparison Table (table)">
      <data key="d6">10.0</data>
      <data key="d7">Entity Carbon-Fiber Spikes belongs to MMLongBench Performance Comparison Table (table)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-bed5ba3a860481b1c29ee139ebb94960</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918250</data>
    </edge>
    <edge source="Graph-Enhanced Retrieval-Augmented Generation" target="GraphRAG">
      <data key="d6">1.0</data>
      <data key="d7">GraphRAG represents an important step in the evolution of Graph-Enhanced Retrieval-Augmented Generation frameworks.</data>
      <data key="d8">development lineage,evolutionary step</data>
      <data key="d9">chunk-b05cce72ca9f7c1081afa465d0eb6116</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918076</data>
    </edge>
    <edge source="Graph-Enhanced Retrieval-Augmented Generation" target="Multimodal Retrieval-Augmented Generation">
      <data key="d6">1.0</data>
      <data key="d7">Multimodal Retrieval-Augmented Generation is an evolution from traditional RAG systems specifically addressing multimodal needs through integrated knowledge.</data>
      <data key="d8">integrated knowledge,system evolution</data>
      <data key="d9">chunk-b05cce72ca9f7c1081afa465d0eb6116</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918077</data>
    </edge>
    <edge source="GraphRAG" target="Edge et al.">
      <data key="d6">1.0</data>
      <data key="d7">Edge et al. contributed to the development and foundation of the GraphRAG framework.</data>
      <data key="d8">framework development,research contribution</data>
      <data key="d9">chunk-b05cce72ca9f7c1081afa465d0eb6116</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918077</data>
    </edge>
    <edge source="Appendix A.5" target="multimodal RAG systems">
      <data key="d6">1.0</data>
      <data key="d7">Appendix A.5 offers an examination of challenges faced by current multimodal RAG systems through systematic failure case analysis.</data>
      <data key="d8">challenges analysis,failure case</data>
      <data key="d9">chunk-b05cce72ca9f7c1081afa465d0eb6116</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918080</data>
    </edge>
    <edge source="Appendix A.5" target="Multimodal RAG Systems">
      <data key="d6">1.0</data>
      <data key="d7">Appendix A.5 analyzes and reveals critical challenges faced by multimodal RAG systems.</data>
      <data key="d8">analysis content,critical challenges</data>
      <data key="d9">chunk-8a30975c53ad9871d2711abf552b151f</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918082</data>
    </edge>
    <edge source="Zhang et al." target="Large Language Models">
      <data key="d6">1.0</data>
      <data key="d7">Zhang et al. provide insights into the limitations of large language models related to long-context inputs and multi-hop queries.</data>
      <data key="d8">limitations identification,research insights</data>
      <data key="d9">chunk-b05cce72ca9f7c1081afa465d0eb6116</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918078</data>
    </edge>
    <edge source="Zhang et al." target="Retrieval-Augmented Generation (RAG)">
      <data key="d6">1.0</data>
      <data key="d7">Zhang et al. are referenced as contributors to the foundational concepts of Retrieval-Augmented Generation (RAG).</data>
      <data key="d8">foundational theory,research contribution</data>
      <data key="d9">chunk-47439f39943c500ebbd82430fcd5a47f</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918096</data>
    </edge>
    <edge source="Bei et al." target="retrieval accuracy">
      <data key="d6">1.0</data>
      <data key="d7">Bei et al. found that graph structures improve both retrieval efficiency and reasoning accuracy within document systems.</data>
      <data key="d8">improvements,research findings</data>
      <data key="d9">chunk-b05cce72ca9f7c1081afa465d0eb6116</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918078</data>
    </edge>
    <edge source="Mavromatis &amp; Karypis" target="GNN-RAG">
      <data key="d6">1.0</data>
      <data key="d7">Mavromatis &amp; Karypis made contributions to neural models like GNN-RAG for enhanced document retrieval.</data>
      <data key="d8">neural modeling,research contribution</data>
      <data key="d9">chunk-b05cce72ca9f7c1081afa465d0eb6116</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918079</data>
    </edge>
    <edge source="Jimenez Gutierrez et al." target="HippoRAG">
      <data key="d6">1.0</data>
      <data key="d7">Jimenez Gutierrez et al. advanced retrieval performance through the development of HippoRAG as a memory-augmented variant.</data>
      <data key="d8">memory augmentation,research advancement</data>
      <data key="d9">chunk-b05cce72ca9f7c1081afa465d0eb6116</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918079</data>
    </edge>
    <edge source="Sarthi et al." target="RAPTOR">
      <data key="d6">1.0</data>
      <data key="d7">Sarthi et al.'s RAPTOR method integrates information for enhancing multi-level reasoning in retrieval systems.</data>
      <data key="d8">hierarchical reasoning,knowledge aggregation</data>
      <data key="d9">chunk-b05cce72ca9f7c1081afa465d0eb6116</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918079</data>
    </edge>
    <edge source="Wang et al." target="ArchRAG">
      <data key="d6">1.0</data>
      <data key="d7">Wang et al. contributed to the knowledge aggregation approaches within the retrieval-augmented generation framework through ArchRAG.</data>
      <data key="d8">framework development,knowledge integration</data>
      <data key="d9">chunk-b05cce72ca9f7c1081afa465d0eb6116</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918080</data>
    </edge>
    <edge source="VideoRAG" target="visual understanding">
      <data key="d6">1.0</data>
      <data key="d7">VideoRAG relies on dual-channel architectures for visual understanding but may sacrifice vital information during text conversion.</data>
      <data key="d8">dual-channel architecture,method reliance</data>
      <data key="d9">chunk-b05cce72ca9f7c1081afa465d0eb6116</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918081</data>
    </edge>
    <edge source="VisRAG" target="document layouts">
      <data key="d6">1.0</data>
      <data key="d7">VisRAG preserves document layouts as images but does not capture detailed relationships within the content accurately.</data>
      <data key="d8">preservation,relationship capture</data>
      <data key="d9">chunk-b05cce72ca9f7c1081afa465d0eb6116</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918081</data>
    </edge>
    <edge source="Multimodal Knowledge Graph" target="Cross-Modal Knowledge Graph">
      <data key="d6">1.0</data>
      <data key="d7">The Cross-Modal Knowledge Graph integrates various non-textual modalities into structured entities within the Multimodal Knowledge Graph.</data>
      <data key="d8">graph structure,knowledge integration</data>
      <data key="d9">chunk-fb3b2a9642d9402ce461122c98a81fd6</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918083</data>
    </edge>
    <edge source="Multimodal Knowledge Graph" target="Graph Representation">
      <data key="d6">1.0</data>
      <data key="d7">Graph Representation serves to create a Multimodal Knowledge Graph that integrates various data types effectively.</data>
      <data key="d8">data integration,framework utility</data>
      <data key="d9">chunk-1b0413a81536bbb879f772d32b8c0dfe</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918272</data>
    </edge>
    <edge source="Multimodal Knowledge Graph" target="Belongs To Edges">
      <data key="d6">1.0</data>
      <data key="d7">Belongs To Edges define the affiliations between entities in a Multimodal Knowledge Graph.</data>
      <data key="d8">entity affiliation,relationship definition</data>
      <data key="d9">chunk-1b0413a81536bbb879f772d32b8c0dfe</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918273</data>
    </edge>
    <edge source="Multimodal Knowledge Graph" target="Semantic Search">
      <data key="d6">1.0</data>
      <data key="d7">The Multimodal Knowledge Graph has essential applications in Semantic Search for improved information retrieval.</data>
      <data key="d8">application,information retrieval</data>
      <data key="d9">chunk-1b0413a81536bbb879f772d32b8c0dfe</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918273</data>
    </edge>
    <edge source="Multimodal Knowledge Graph" target="Natural Language Processing">
      <data key="d6">1.0</data>
      <data key="d7">The Multimodal Knowledge Graph is vital for Natural Language Processing, particularly in understanding complex relationships across modalities.</data>
      <data key="d8">data handling,relationship understanding</data>
      <data key="d9">chunk-1b0413a81536bbb879f772d32b8c0dfe</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918274</data>
    </edge>
    <edge source="Multimodal Knowledge Graph" target="Multimodal Graph Construction Equation (equation)">
      <data key="d6">10.0</data>
      <data key="d7">Entity Multimodal Knowledge Graph belongs to Multimodal Graph Construction Equation (equation)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-1b0413a81536bbb879f772d32b8c0dfe</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918279</data>
    </edge>
    <edge source="Text-Based Knowledge Graph" target="Dual-Graph Construction for Multimodal Knowledge">
      <data key="d6">1.0</data>
      <data key="d7">Dual-Graph Construction for Multimodal Knowledge results in the creation of a Text-Based Knowledge Graph showing related nodes.</data>
      <data key="d8">data representation,graph data</data>
      <data key="d9">chunk-92698bb9a611d4820a4578e37e74e670</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918226</data>
    </edge>
    <edge source="Text-Based Knowledge Graph" target="RAG-Anything Framework Overview (image)">
      <data key="d6">10.0</data>
      <data key="d7">Entity Text-Based Knowledge Graph belongs to RAG-Anything Framework Overview (image)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-92698bb9a611d4820a4578e37e74e670</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918231</data>
    </edge>
    <edge source="Cross-Modal Knowledge Graph" target="Dual-Graph Construction for Multimodal Knowledge">
      <data key="d6">1.0</data>
      <data key="d7">The output of Dual-Graph Construction includes a Cross-Modal Knowledge Graph that visualizes interrelated data nodes.</data>
      <data key="d8">data interrelation,graph creation</data>
      <data key="d9">chunk-92698bb9a611d4820a4578e37e74e670</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918227</data>
    </edge>
    <edge source="Cross-Modal Knowledge Graph" target="RAG-Anything Framework Overview (image)">
      <data key="d6">10.0</data>
      <data key="d7">Entity Cross-Modal Knowledge Graph belongs to RAG-Anything Framework Overview (image)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-92698bb9a611d4820a4578e37e74e670</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918231</data>
    </edge>
    <edge source="Entity Alignment" target="Graph Fusion">
      <data key="d6">1.0</data>
      <data key="d7">Entity Alignment facilitates the merging of different graph structures through Graph Fusion by identifying semantically equivalent entities.</data>
      <data key="d8">entity matching,integration method</data>
      <data key="d9">chunk-fb3b2a9642d9402ce461122c98a81fd6</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918082</data>
    </edge>
    <edge source="Graph Fusion" target="Embedding Table">
      <data key="d6">1.0</data>
      <data key="d7">The Embedding Table aids in Graph Fusion by providing dense representations essential for efficient retrieval across multimodal entities.</data>
      <data key="d8">data representation,retrieval efficiency</data>
      <data key="d9">chunk-fb3b2a9642d9402ce461122c98a81fd6</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918082</data>
    </edge>
    <edge source="Embedding Table" target="Retrieval Stage">
      <data key="d6">1.0</data>
      <data key="d7">The Embedding Table is utilized during the Retrieval Stage to aid in knowledge representation and retrieval efficiency.</data>
      <data key="d8">component,knowledge representation</data>
      <data key="d9">chunk-50ef79f5820dd68dd3803e9eecf98e80</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918089</data>
    </edge>
    <edge source="Multimodal RAG Systems" target="Text-Centric Retrieval Bias">
      <data key="d6">1.0</data>
      <data key="d7">Text-centric retrieval bias is identified as a limitation affecting multimodal RAG systems.</data>
      <data key="d8">retrieval issues,system limitation</data>
      <data key="d9">chunk-8a30975c53ad9871d2711abf552b151f</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918083</data>
    </edge>
    <edge source="Multimodal RAG Systems" target="Cross-Modal Misalignment">
      <data key="d6">1.0</data>
      <data key="d7">Cross-modal misalignment scenarios highlight a limitation of multimodal RAG systems.</data>
      <data key="d8">information mismatch,system limitation</data>
      <data key="d9">chunk-8a30975c53ad9871d2711abf552b151f</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918084</data>
    </edge>
    <edge source="Multimodal RAG Systems" target="Adaptive Spatial Reasoning">
      <data key="d6">1.0</data>
      <data key="d7">Adaptive spatial reasoning is suggested as a solution to address complex document handling in multimodal RAG systems.</data>
      <data key="d8">document complexity,proposed solution</data>
      <data key="d9">chunk-8a30975c53ad9871d2711abf552b151f</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918085</data>
    </edge>
    <edge source="Multimodal RAG Systems" target="Layout-Aware Parsing Mechanisms">
      <data key="d6">1.0</data>
      <data key="d7">Layout-aware parsing mechanisms are recommended enhancements for effectively processing complex multimodal documents.</data>
      <data key="d8">document processing,proposed enhancement</data>
      <data key="d9">chunk-8a30975c53ad9871d2711abf552b151f</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918086</data>
    </edge>
    <edge source="Multimodal RAG Systems" target="Rigid Spatial Processing Patterns">
      <data key="d6">1.0</data>
      <data key="d7">Rigid spatial processing patterns are identified as limitations in multimodal RAG systems, failing to adapt to non-standard document layouts.</data>
      <data key="d8">processing issues,system limitation</data>
      <data key="d9">chunk-8a30975c53ad9871d2711abf552b151f</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918086</data>
    </edge>
    <edge source="Multimodal RAG Systems" target="Systematic Failure Case Examination">
      <data key="d6">1.0</data>
      <data key="d7">Systematic failure case examination is a method used to identify issues within multimodal RAG systems.</data>
      <data key="d8">analysis method,issue identification</data>
      <data key="d9">chunk-8a30975c53ad9871d2711abf552b151f</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918088</data>
    </edge>
    <edge source="Multimodal Knowledge Unification" target="RAG-Anything Framework Overview (image)">
      <data key="d6">10.0</data>
      <data key="d7">Entity Multimodal Knowledge Unification belongs to RAG-Anything Framework Overview (image)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-92698bb9a611d4820a4578e37e74e670</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918227</data>
    </edge>
    <edge source="Cross-Modal Hybrid Retrieval" target="Retrieval Stage">
      <data key="d6">1.0</data>
      <data key="d7">Cross-Modal Hybrid Retrieval provides a method for retrieval operations on multimodal documents during the Retrieval Stage.</data>
      <data key="d8">framework,process</data>
      <data key="d9">chunk-50ef79f5820dd68dd3803e9eecf98e80</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918088</data>
    </edge>
    <edge source="Cross-Modal Hybrid Retrieval" target="Unified Knowledge Graph">
      <data key="d6">1.0</data>
      <data key="d7">The Unified Knowledge Graph is central to the Cross-Modal Hybrid Retrieval framework, ensuring effective access to structured knowledge.</data>
      <data key="d8">information access,structured knowledge</data>
      <data key="d9">chunk-50ef79f5820dd68dd3803e9eecf98e80</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918090</data>
    </edge>
    <edge source="Large Language Models (LLMs)" target="Current RAG Systems">
      <data key="d6">1.0</data>
      <data key="d7">Current RAG Systems highlight the limitations of LLMs regarding static knowledge that needs to be expanded through retrieval mechanisms.</data>
      <data key="d8">knowledge retrieval,limitation</data>
      <data key="d9">chunk-2342fe853447858443171ce27b6bf554</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918085</data>
    </edge>
    <edge source="Large Language Models (LLMs)" target="Retrieval-Augmented Generation (RAG)">
      <data key="d6">1.0</data>
      <data key="d7">Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by enabling them to adapt and utilize external knowledge in their responses.</data>
      <data key="d8">adaptation,knowledge enhancement</data>
      <data key="d9">chunk-2342fe853447858443171ce27b6bf554</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918085</data>
    </edge>
    <edge source="Novo Nordisk" target="wages and salaries">
      <data key="d6">1.0</data>
      <data key="d7">Novo Nordisk's financial reporting includes metrics regarding their spending on wages and salaries.</data>
      <data key="d8">expenditure,financial analysis</data>
      <data key="d9">chunk-ea853774b3241b5f39f26edf475d4bd5</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918086</data>
    </edge>
    <edge source="Figure 3" target="Multimodal Document with t-SNE Analysis (image)">
      <data key="d6">10.0</data>
      <data key="d7">Entity Figure 3 belongs to Multimodal Document with t-SNE Analysis (image)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-76f41ba1f43d29b4684e8f0113c3efae</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918236</data>
    </edge>
    <edge source="Retrieval Stage" target="Knowledge Graph">
      <data key="d6">1.0</data>
      <data key="d7">The Knowledge Graph provides structural support during the Retrieval Stage, facilitating the identification of relevant knowledge.</data>
      <data key="d8">retrieval process,structural support</data>
      <data key="d9">chunk-50ef79f5820dd68dd3803e9eecf98e80</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918090</data>
    </edge>
    <edge source="User Query" target="Modality-Aware Query Encoding">
      <data key="d6">1.0</data>
      <data key="d7">User Query is analyzed using Modality-Aware Query Encoding to enhance retrieval effectiveness.</data>
      <data key="d8">input analysis,query enhancement</data>
      <data key="d9">chunk-50ef79f5820dd68dd3803e9eecf98e80</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918088</data>
    </edge>
    <edge source="Hybrid Knowledge Retrieval Architecture" target="Structural Knowledge Navigation">
      <data key="d6">1.0</data>
      <data key="d7">Hybrid Knowledge Retrieval Architecture combines Structural Knowledge Navigation and Semantic Similarity Matching for improved knowledge retrieval.</data>
      <data key="d8">complementary mechanisms,knowledge retrieval</data>
      <data key="d9">chunk-50ef79f5820dd68dd3803e9eecf98e80</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918089</data>
    </edge>
    <edge source="Structural Knowledge Navigation" target="Candidate Pool">
      <data key="d6">1.0</data>
      <data key="d7">Structural Knowledge Navigation contributes to identifying relevant knowledge candidates in the Candidate Pool.</data>
      <data key="d8">extraction,knowledge relevance</data>
      <data key="d9">chunk-50ef79f5820dd68dd3803e9eecf98e80</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918089</data>
    </edge>
    <edge source="Semantic Similarity Matching" target="Candidate Pool">
      <data key="d6">1.0</data>
      <data key="d7">Semantic Similarity Matching aids in identifying semantically relevant candidates for inclusion in the Candidate Pool.</data>
      <data key="d8">extraction,knowledge relevance</data>
      <data key="d9">chunk-50ef79f5820dd68dd3803e9eecf98e80</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918090</data>
    </edge>
    <edge source="Candidate Pool" target="Multi-Signal Fusion Scoring">
      <data key="d6">1.0</data>
      <data key="d7">Candidate Pool is ranked using Multi-Signal Fusion Scoring to balance structural and semantic relevance among retrieval candidates.</data>
      <data key="d8">ranking mechanism,retrieval candidates</data>
      <data key="d9">chunk-50ef79f5820dd68dd3803e9eecf98e80</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918091</data>
    </edge>
    <edge source="Candidate Pool" target="Cosine Similarity">
      <data key="d6">1.0</data>
      <data key="d7">Cosine Similarity is applied to the Candidate Pool to rank the most semantically similar knowledge chunks during retrieval.</data>
      <data key="d8">ranking method,semantic matching</data>
      <data key="d9">chunk-50ef79f5820dd68dd3803e9eecf98e80</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918092</data>
    </edge>
    <edge source="Decomposition of Multimodal Knowledge Sources (equation)" target="Mathematical Equation">
      <data key="d6">10.0</data>
      <data key="d7">Entity Mathematical Equation belongs to Decomposition of Multimodal Knowledge Sources (equation)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-88ebe14996977f568b531a51ff2b02e2</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918205</data>
    </edge>
    <edge source="Decomposition of Multimodal Knowledge Sources (equation)" target="Knowledge Source">
      <data key="d6">10.0</data>
      <data key="d7">Entity Knowledge Source belongs to Decomposition of Multimodal Knowledge Sources (equation)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-88ebe14996977f568b531a51ff2b02e2</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918206</data>
    </edge>
    <edge source="Decomposition of Multimodal Knowledge Sources (equation)" target="RAG-Anything Framework">
      <data key="d6">10.0</data>
      <data key="d7">Entity RAG-Anything Framework belongs to Decomposition of Multimodal Knowledge Sources (equation)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-88ebe14996977f568b531a51ff2b02e2</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918208</data>
    </edge>
    <edge source="Decomposition of Multimodal Knowledge Sources (equation)" target="Multimodal Corpus">
      <data key="d6">10.0</data>
      <data key="d7">Entity Multimodal Corpus belongs to Decomposition of Multimodal Knowledge Sources (equation)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-88ebe14996977f568b531a51ff2b02e2</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918209</data>
    </edge>
    <edge source="Decomposition of Multimodal Knowledge Sources (equation)" target="Atomic Content Units">
      <data key="d6">10.0</data>
      <data key="d7">Entity Atomic Content Units belongs to Decomposition of Multimodal Knowledge Sources (equation)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-88ebe14996977f568b531a51ff2b02e2</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918209</data>
    </edge>
    <edge source="Decomposition of Multimodal Knowledge Sources (equation)" target="Modality Type">
      <data key="d6">10.0</data>
      <data key="d7">Entity Modality Type belongs to Decomposition of Multimodal Knowledge Sources (equation)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-88ebe14996977f568b531a51ff2b02e2</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918210</data>
    </edge>
    <edge source="Decomposition of Multimodal Knowledge Sources (equation)" target="Decompose">
      <data key="d6">10.0</data>
      <data key="d7">Entity Decompose belongs to Decomposition of Multimodal Knowledge Sources (equation)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-88ebe14996977f568b531a51ff2b02e2</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918211</data>
    </edge>
    <edge source="RAG-Anything Framework Overview (image)" target="Retrieval and Response Generation">
      <data key="d6">10.0</data>
      <data key="d7">Entity Retrieval and Response Generation belongs to RAG-Anything Framework Overview (image)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-92698bb9a611d4820a4578e37e74e670</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918228</data>
    </edge>
    <edge source="RAG-Anything Framework Overview (image)" target="VLM/LLM">
      <data key="d6">10.0</data>
      <data key="d7">Entity VLM/LLM belongs to RAG-Anything Framework Overview (image)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-92698bb9a611d4820a4578e37e74e670</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918229</data>
    </edge>
    <edge source="RAG-Anything Framework Overview (image)" target="Dual-Graph Construction for Multimodal Knowledge">
      <data key="d6">10.0</data>
      <data key="d7">Entity Dual-Graph Construction for Multimodal Knowledge belongs to RAG-Anything Framework Overview (image)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-92698bb9a611d4820a4578e37e74e670</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918230</data>
    </edge>
    <edge source="RAG-Anything Framework Overview (image)" target="Vector Database">
      <data key="d6">10.0</data>
      <data key="d7">Entity Vector Database belongs to RAG-Anything Framework Overview (image)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-92698bb9a611d4820a4578e37e74e670</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918232</data>
    </edge>
    <edge source="RAG-Anything Framework Overview (image)" target="Query Processing">
      <data key="d6">10.0</data>
      <data key="d7">Entity Query Processing belongs to RAG-Anything Framework Overview (image)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-92698bb9a611d4820a4578e37e74e670</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918233</data>
    </edge>
    <edge source="RAG-Anything Framework Overview (image)" target="Structured Content List">
      <data key="d6">10.0</data>
      <data key="d7">Entity Structured Content List belongs to RAG-Anything Framework Overview (image)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-92698bb9a611d4820a4578e37e74e670</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918233</data>
    </edge>
    <edge source="Dual-Graph Construction Mapping (equation)" target="Mathematical Equation Analysis">
      <data key="d6">10.0</data>
      <data key="d7">Entity Mathematical Equation Analysis belongs to Dual-Graph Construction Mapping (equation)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-2d23b106580d44e93ef9110173f0cfaf</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918208</data>
    </edge>
    <edge source="Dual-Graph Construction Mapping (equation)" target="Equation">
      <data key="d6">10.0</data>
      <data key="d7">Entity Equation belongs to Dual-Graph Construction Mapping (equation)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-2d23b106580d44e93ef9110173f0cfaf</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918209</data>
    </edge>
    <edge source="Dual-Graph Construction Mapping (equation)" target="Graph Structure">
      <data key="d6">10.0</data>
      <data key="d7">Entity Graph Structure belongs to Dual-Graph Construction Mapping (equation)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-2d23b106580d44e93ef9110173f0cfaf</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918209</data>
    </edge>
    <edge source="Dual-Graph Construction Mapping (equation)" target="Set of Vertices">
      <data key="d6">10.0</data>
      <data key="d7">Entity Set of Vertices belongs to Dual-Graph Construction Mapping (equation)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-2d23b106580d44e93ef9110173f0cfaf</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918210</data>
    </edge>
    <edge source="Dual-Graph Construction Mapping (equation)" target="Set of Edges">
      <data key="d6">10.0</data>
      <data key="d7">Entity Set of Edges belongs to Dual-Graph Construction Mapping (equation)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-2d23b106580d44e93ef9110173f0cfaf</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918211</data>
    </edge>
    <edge source="Dual-Graph Construction Mapping (equation)" target="Generated Textual Description">
      <data key="d6">10.0</data>
      <data key="d7">Entity Generated Textual Description belongs to Dual-Graph Construction Mapping (equation)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-2d23b106580d44e93ef9110173f0cfaf</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918212</data>
    </edge>
    <edge source="Dual-Graph Construction Mapping (equation)" target="Dual-Graph Methodology">
      <data key="d6">10.0</data>
      <data key="d7">Entity Dual-Graph Methodology belongs to Dual-Graph Construction Mapping (equation)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-2d23b106580d44e93ef9110173f0cfaf</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918212</data>
    </edge>
    <edge source="Dual-Graph Construction Mapping (equation)" target="RAG-Anything Framework">
      <data key="d6">10.0</data>
      <data key="d7">Entity RAG-Anything Framework belongs to Dual-Graph Construction Mapping (equation)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-2d23b106580d44e93ef9110173f0cfaf</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918213</data>
    </edge>
    <edge source="Dual-Graph Construction Mapping (equation)" target="Cross-Modal Retrieval">
      <data key="d6">10.0</data>
      <data key="d7">Entity Cross-Modal Retrieval belongs to Dual-Graph Construction Mapping (equation)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-2d23b106580d44e93ef9110173f0cfaf</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918214</data>
    </edge>
    <edge source="Dual-Graph Construction Mapping (equation)" target="Structured Knowledge Representation">
      <data key="d6">10.0</data>
      <data key="d7">Entity Structured Knowledge Representation belongs to Dual-Graph Construction Mapping (equation)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-2d23b106580d44e93ef9110173f0cfaf</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918215</data>
    </edge>
    <edge source="Dual-Graph Construction Mapping (equation)" target="Mapping of Content Unit">
      <data key="d6">10.0</data>
      <data key="d7">Entity Mapping of Content Unit belongs to Dual-Graph Construction Mapping (equation)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-2d23b106580d44e93ef9110173f0cfaf</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918216</data>
    </edge>
    <edge source="Dual-Graph Construction Mapping (equation)" target="Function R">
      <data key="d6">10.0</data>
      <data key="d7">Entity Function R belongs to Dual-Graph Construction Mapping (equation)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-2d23b106580d44e93ef9110173f0cfaf</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918216</data>
    </edge>
    <edge source="Multimodal Graph Construction Equation (equation)" target="Multimodal Vertex Entities">
      <data key="d6">10.0</data>
      <data key="d7">Entity Multimodal Vertex Entities belongs to Multimodal Graph Construction Equation (equation)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-1b0413a81536bbb879f772d32b8c0dfe</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918273</data>
    </edge>
    <edge source="Multimodal Graph Construction Equation (equation)" target="Mathematical Equation">
      <data key="d6">10.0</data>
      <data key="d7">Entity Mathematical Equation belongs to Multimodal Graph Construction Equation (equation)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-1b0413a81536bbb879f772d32b8c0dfe</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918274</data>
    </edge>
    <edge source="Multimodal Graph Construction Equation (equation)" target="Graph Representation">
      <data key="d6">10.0</data>
      <data key="d7">Entity Graph Representation belongs to Multimodal Graph Construction Equation (equation)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-1b0413a81536bbb879f772d32b8c0dfe</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918275</data>
    </edge>
    <edge source="Multimodal Graph Construction Equation (equation)" target="Intra-Chunk Entities">
      <data key="d6">10.0</data>
      <data key="d7">Entity Intra-Chunk Entities belongs to Multimodal Graph Construction Equation (equation)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-1b0413a81536bbb879f772d32b8c0dfe</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918277</data>
    </edge>
    <edge source="Multimodal Graph Construction Equation (equation)" target="Belongs To Edges">
      <data key="d6">10.0</data>
      <data key="d7">Entity Belongs To Edges belongs to Multimodal Graph Construction Equation (equation)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-1b0413a81536bbb879f772d32b8c0dfe</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918278</data>
    </edge>
    <edge source="Multimodal Graph Construction Equation (equation)" target="Semantic Search">
      <data key="d6">10.0</data>
      <data key="d7">Entity Semantic Search belongs to Multimodal Graph Construction Equation (equation)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-1b0413a81536bbb879f772d32b8c0dfe</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918279</data>
    </edge>
    <edge source="Multimodal Graph Construction Equation (equation)" target="Natural Language Processing">
      <data key="d6">10.0</data>
      <data key="d7">Entity Natural Language Processing belongs to Multimodal Graph Construction Equation (equation)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-1b0413a81536bbb879f772d32b8c0dfe</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918280</data>
    </edge>
    <edge source="Multimodal Graph Construction Equation (equation)" target="Mathematical Analysis">
      <data key="d6">10.0</data>
      <data key="d7">Entity Mathematical Analysis belongs to Multimodal Graph Construction Equation (equation)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-1b0413a81536bbb879f772d32b8c0dfe</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918281</data>
    </edge>
    <edge source="Unified Embedding Function for Multimodal Knowledge (equation)" target="T">
      <data key="d6">10.0</data>
      <data key="d7">Entity T belongs to Unified Embedding Function for Multimodal Knowledge (equation)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-11b9991810e9963c09e4017983a04dc3</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918235</data>
    </edge>
    <edge source="Unified Embedding Function for Multimodal Knowledge (equation)" target="Mathematical Equation">
      <data key="d6">10.0</data>
      <data key="d7">Entity Mathematical Equation belongs to Unified Embedding Function for Multimodal Knowledge (equation)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-11b9991810e9963c09e4017983a04dc3</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918236</data>
    </edge>
    <edge source="Unified Embedding Function for Multimodal Knowledge (equation)" target="emb">
      <data key="d6">10.0</data>
      <data key="d7">Entity emb belongs to Unified Embedding Function for Multimodal Knowledge (equation)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-11b9991810e9963c09e4017983a04dc3</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918237</data>
    </edge>
    <edge source="Unified Embedding Function for Multimodal Knowledge (equation)" target="V">
      <data key="d6">10.0</data>
      <data key="d7">Entity V belongs to Unified Embedding Function for Multimodal Knowledge (equation)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-11b9991810e9963c09e4017983a04dc3</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918237</data>
    </edge>
    <edge source="Unified Embedding Function for Multimodal Knowledge (equation)" target="E">
      <data key="d6">10.0</data>
      <data key="d7">Entity E belongs to Unified Embedding Function for Multimodal Knowledge (equation)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-11b9991810e9963c09e4017983a04dc3</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918238</data>
    </edge>
    <edge source="Unified Embedding Function for Multimodal Knowledge (equation)" target="c">
      <data key="d6">10.0</data>
      <data key="d7">Entity c belongs to Unified Embedding Function for Multimodal Knowledge (equation)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-11b9991810e9963c09e4017983a04dc3</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918239</data>
    </edge>
    <edge source="Unified Embedding Function for Multimodal Knowledge (equation)" target="Multimodal Retrieval Systems">
      <data key="d6">10.0</data>
      <data key="d7">Entity Multimodal Retrieval Systems belongs to Unified Embedding Function for Multimodal Knowledge (equation)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-11b9991810e9963c09e4017983a04dc3</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918239</data>
    </edge>
    <edge source="Unified Embedding Function for Multimodal Knowledge (equation)" target="Atom Content Chunks">
      <data key="d6">10.0</data>
      <data key="d7">Entity Atom Content Chunks belongs to Unified Embedding Function for Multimodal Knowledge (equation)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-11b9991810e9963c09e4017983a04dc3</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918240</data>
    </edge>
    <edge source="Unified Embedding Function for Multimodal Knowledge (equation)" target="Similarity Searches">
      <data key="d6">10.0</data>
      <data key="d7">Entity Similarity Searches belongs to Unified Embedding Function for Multimodal Knowledge (equation)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-11b9991810e9963c09e4017983a04dc3</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918241</data>
    </edge>
    <edge source="Statistics of Experimental Datasets (table)" target="Table 1">
      <data key="d6">10.0</data>
      <data key="d7">Entity Table 1 belongs to Statistics of Experimental Datasets (table)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-3dd00be716f65d39ed28040e8449c044</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918253</data>
    </edge>
    <edge source="Statistics of Experimental Datasets (table)" target="Statistics of Experimental Datasets">
      <data key="d6">10.0</data>
      <data key="d7">Entity Statistics of Experimental Datasets belongs to Statistics of Experimental Datasets (table)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-3dd00be716f65d39ed28040e8449c044</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918254</data>
    </edge>
    <edge source="Multimodal Language Model Response Function (equation)" target="Response">
      <data key="d6">10.0</data>
      <data key="d7">Entity Response belongs to Multimodal Language Model Response Function (equation)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-3fde5aaf8f821b4be0b48eb6bfb98337</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918270</data>
    </edge>
    <edge source="Multimodal Language Model Response Function (equation)" target="VLM">
      <data key="d6">10.0</data>
      <data key="d7">Entity VLM belongs to Multimodal Language Model Response Function (equation)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-3fde5aaf8f821b4be0b48eb6bfb98337</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918271</data>
    </edge>
    <edge source="Multimodal Language Model Response Function (equation)" target="q">
      <data key="d6">10.0</data>
      <data key="d7">Entity q belongs to Multimodal Language Model Response Function (equation)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-3fde5aaf8f821b4be0b48eb6bfb98337</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918271</data>
    </edge>
    <edge source="Multimodal Language Model Response Function (equation)" target="P(q)">
      <data key="d6">10.0</data>
      <data key="d7">Entity P(q) belongs to Multimodal Language Model Response Function (equation)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-3fde5aaf8f821b4be0b48eb6bfb98337</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918272</data>
    </edge>
    <edge source="Multimodal Language Model Response Function (equation)" target="V*(q)">
      <data key="d6">10.0</data>
      <data key="d7">Entity V*(q) belongs to Multimodal Language Model Response Function (equation)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-3fde5aaf8f821b4be0b48eb6bfb98337</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918273</data>
    </edge>
    <edge source="Multimodal Language Model Response Function (equation)" target="Hybrid Retrieval Mechanisms">
      <data key="d6">10.0</data>
      <data key="d7">Entity Hybrid Retrieval Mechanisms belongs to Multimodal Language Model Response Function (equation)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-3fde5aaf8f821b4be0b48eb6bfb98337</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918273</data>
    </edge>
    <edge source="Multimodal Language Model Response Function (equation)" target="Advanced AI Systems">
      <data key="d6">10.0</data>
      <data key="d7">Entity Advanced AI Systems belongs to Multimodal Language Model Response Function (equation)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-3fde5aaf8f821b4be0b48eb6bfb98337</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918274</data>
    </edge>
    <edge source="Multimodal Language Model Response Function (equation)" target="Multimodal Question-Answering Systems">
      <data key="d6">10.0</data>
      <data key="d7">Entity Multimodal Question-Answering Systems belongs to Multimodal Language Model Response Function (equation)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-3fde5aaf8f821b4be0b48eb6bfb98337</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918275</data>
    </edge>
    <edge source="DocBench Multimodal Performance Table (table)" target="RAGAnything">
      <data key="d6">10.0</data>
      <data key="d7">Entity RAGAnything belongs to DocBench Multimodal Performance Table (table)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-c74c755aa200873b54b093c4aef3485e</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918221</data>
    </edge>
    <edge source="DocBench Multimodal Performance Table (table)" target="Academia">
      <data key="d6">10.0</data>
      <data key="d7">Entity Academia belongs to DocBench Multimodal Performance Table (table)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-c74c755aa200873b54b093c4aef3485e</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918222</data>
    </edge>
    <edge source="DocBench Multimodal Performance Table (table)" target="DocBench Dataset">
      <data key="d6">10.0</data>
      <data key="d7">Entity DocBench Dataset belongs to DocBench Multimodal Performance Table (table)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-c74c755aa200873b54b093c4aef3485e</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918223</data>
    </edge>
    <edge source="DocBench Multimodal Performance Table (table)" target="Finance">
      <data key="d6">10.0</data>
      <data key="d7">Entity Finance belongs to DocBench Multimodal Performance Table (table)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-c74c755aa200873b54b093c4aef3485e</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918223</data>
    </edge>
    <edge source="DocBench Multimodal Performance Table (table)" target="Government">
      <data key="d6">10.0</data>
      <data key="d7">Entity Government belongs to DocBench Multimodal Performance Table (table)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-c74c755aa200873b54b093c4aef3485e</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918224</data>
    </edge>
    <edge source="DocBench Multimodal Performance Table (table)" target="Law">
      <data key="d6">10.0</data>
      <data key="d7">Entity Law belongs to DocBench Multimodal Performance Table (table)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-c74c755aa200873b54b093c4aef3485e</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918225</data>
    </edge>
    <edge source="DocBench Multimodal Performance Table (table)" target="News">
      <data key="d6">10.0</data>
      <data key="d7">Entity News belongs to DocBench Multimodal Performance Table (table)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-c74c755aa200873b54b093c4aef3485e</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918226</data>
    </edge>
    <edge source="DocBench Multimodal Performance Table (table)" target="Text-only">
      <data key="d6">10.0</data>
      <data key="d7">Entity Text-only belongs to DocBench Multimodal Performance Table (table)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-c74c755aa200873b54b093c4aef3485e</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918227</data>
    </edge>
    <edge source="DocBench Multimodal Performance Table (table)" target="Multimodal">
      <data key="d6">10.0</data>
      <data key="d7">Entity Multimodal belongs to DocBench Multimodal Performance Table (table)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-c74c755aa200873b54b093c4aef3485e</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918227</data>
    </edge>
    <edge source="DocBench Multimodal Performance Table (table)" target="Unanswerable Queries">
      <data key="d6">10.0</data>
      <data key="d7">Entity Unanswerable Queries belongs to DocBench Multimodal Performance Table (table)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-c74c755aa200873b54b093c4aef3485e</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918228</data>
    </edge>
    <edge source="DocBench Multimodal Performance Table (table)" target="Overall Accuracy">
      <data key="d6">10.0</data>
      <data key="d7">Entity Overall Accuracy belongs to DocBench Multimodal Performance Table (table)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-c74c755aa200873b54b093c4aef3485e</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918229</data>
    </edge>
    <edge source="MMLongBench Performance Comparison Table (table)" target="GPT-40-mini">
      <data key="d6">10.0</data>
      <data key="d7">Entity GPT-40-mini belongs to MMLongBench Performance Comparison Table (table)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-bed5ba3a860481b1c29ee139ebb94960</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918240</data>
    </edge>
    <edge source="MMLongBench Performance Comparison Table (table)" target="Research Reports/Introductions">
      <data key="d6">10.0</data>
      <data key="d7">Entity Research Reports/Introductions belongs to MMLongBench Performance Comparison Table (table)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-bed5ba3a860481b1c29ee139ebb94960</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918242</data>
    </edge>
    <edge source="MMLongBench Performance Comparison Table (table)" target="Tutorials/Workshops">
      <data key="d6">10.0</data>
      <data key="d7">Entity Tutorials/Workshops belongs to MMLongBench Performance Comparison Table (table)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-bed5ba3a860481b1c29ee139ebb94960</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918243</data>
    </edge>
    <edge source="MMLongBench Performance Comparison Table (table)" target="Academic Papers">
      <data key="d6">10.0</data>
      <data key="d7">Entity Academic Papers belongs to MMLongBench Performance Comparison Table (table)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-bed5ba3a860481b1c29ee139ebb94960</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918244</data>
    </edge>
    <edge source="MMLongBench Performance Comparison Table (table)" target="Guidebooks">
      <data key="d6">10.0</data>
      <data key="d7">Entity Guidebooks belongs to MMLongBench Performance Comparison Table (table)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-bed5ba3a860481b1c29ee139ebb94960</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918245</data>
    </edge>
    <edge source="MMLongBench Performance Comparison Table (table)" target="Brochures">
      <data key="d6">10.0</data>
      <data key="d7">Entity Brochures belongs to MMLongBench Performance Comparison Table (table)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-bed5ba3a860481b1c29ee139ebb94960</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918245</data>
    </edge>
    <edge source="MMLongBench Performance Comparison Table (table)" target="Administration/Industry Files">
      <data key="d6">10.0</data>
      <data key="d7">Entity Administration/Industry Files belongs to MMLongBench Performance Comparison Table (table)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-bed5ba3a860481b1c29ee139ebb94960</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918247</data>
    </edge>
    <edge source="MMLongBench Performance Comparison Table (table)" target="Financial Reports">
      <data key="d6">10.0</data>
      <data key="d7">Entity Financial Reports belongs to MMLongBench Performance Comparison Table (table)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-bed5ba3a860481b1c29ee139ebb94960</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918248</data>
    </edge>
    <edge source="MMLongBench Performance Comparison Table (table)" target="Table 3">
      <data key="d6">10.0</data>
      <data key="d7">Entity Table 3 belongs to MMLongBench Performance Comparison Table (table)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-bed5ba3a860481b1c29ee139ebb94960</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918248</data>
    </edge>
    <edge source="MMLongBench Performance Comparison Table (table)" target="Accuracy">
      <data key="d6">10.0</data>
      <data key="d7">Entity Accuracy belongs to MMLongBench Performance Comparison Table (table)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-bed5ba3a860481b1c29ee139ebb94960</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918249</data>
    </edge>
    <edge source="Performance Evaluation Charts (image)" target="RAGAnything">
      <data key="d6">10.0</data>
      <data key="d7">Entity RAGAnything belongs to Performance Evaluation Charts (image)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-0ed9b2ed0a4bc6dedef04c5c1a224400</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918214</data>
    </edge>
    <edge source="Performance Evaluation Charts (image)" target="DocBench QA Counts">
      <data key="d6">10.0</data>
      <data key="d7">Entity DocBench QA Counts belongs to Performance Evaluation Charts (image)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-0ed9b2ed0a4bc6dedef04c5c1a224400</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918216</data>
    </edge>
    <edge source="Performance Evaluation Charts (image)" target="MMLongBench Accuracy">
      <data key="d6">10.0</data>
      <data key="d7">Entity MMLongBench Accuracy belongs to Performance Evaluation Charts (image)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-0ed9b2ed0a4bc6dedef04c5c1a224400</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918216</data>
    </edge>
    <edge source="Performance Evaluation Charts (image)" target="MMLongBench QA Counts">
      <data key="d6">10.0</data>
      <data key="d7">Entity MMLongBench QA Counts belongs to Performance Evaluation Charts (image)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-0ed9b2ed0a4bc6dedef04c5c1a224400</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918217</data>
    </edge>
    <edge source="Performance Evaluation Charts (image)" target="Four Separate Charts">
      <data key="d6">10.0</data>
      <data key="d7">Entity Four Separate Charts belongs to Performance Evaluation Charts (image)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-0ed9b2ed0a4bc6dedef04c5c1a224400</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918218</data>
    </edge>
    <edge source="Performance Evaluation Charts (image)" target="DocBench Accuracy">
      <data key="d6">10.0</data>
      <data key="d7">Entity DocBench Accuracy belongs to Performance Evaluation Charts (image)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-0ed9b2ed0a4bc6dedef04c5c1a224400</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918219</data>
    </edge>
    <edge source="Performance Evaluation Charts (image)" target="Page Range Categories">
      <data key="d6">10.0</data>
      <data key="d7">Entity Page Range Categories belongs to Performance Evaluation Charts (image)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-0ed9b2ed0a4bc6dedef04c5c1a224400</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918219</data>
    </edge>
    <edge source="Ablation Study Results on DocBench (table)" target="Ablation Study">
      <data key="d6">10.0</data>
      <data key="d7">Entity Ablation Study belongs to Ablation Study Results on DocBench (table)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-260174f5a5690a7e51bdf14e210da8b4</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918259</data>
    </edge>
    <edge source="Ablation Study Results on DocBench (table)" target="Chunk-Only">
      <data key="d6">10.0</data>
      <data key="d7">Entity Chunk-Only belongs to Ablation Study Results on DocBench (table)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-260174f5a5690a7e51bdf14e210da8b4</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918260</data>
    </edge>
    <edge source="Ablation Study Results on DocBench (table)" target="W/O Reranker">
      <data key="d6">10.0</data>
      <data key="d7">Entity W/O Reranker belongs to Ablation Study Results on DocBench (table)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-260174f5a5690a7e51bdf14e210da8b4</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918260</data>
    </edge>
    <edge source="Ablation Study Results on DocBench (table)" target="Performance Metrics">
      <data key="d6">10.0</data>
      <data key="d7">Entity Performance Metrics belongs to Ablation Study Results on DocBench (table)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-260174f5a5690a7e51bdf14e210da8b4</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918263</data>
    </edge>
    <edge source="Ablation Study Results on DocBench (table)" target="Academic Domain">
      <data key="d6">10.0</data>
      <data key="d7">Entity Academic Domain belongs to Ablation Study Results on DocBench (table)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-260174f5a5690a7e51bdf14e210da8b4</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918263</data>
    </edge>
    <edge source="Ablation Study Results on DocBench (table)" target="Financial Domain">
      <data key="d6">10.0</data>
      <data key="d7">Entity Financial Domain belongs to Ablation Study Results on DocBench (table)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-260174f5a5690a7e51bdf14e210da8b4</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918264</data>
    </edge>
    <edge source="Ablation Study Results on DocBench (table)" target="Governmental Domain">
      <data key="d6">10.0</data>
      <data key="d7">Entity Governmental Domain belongs to Ablation Study Results on DocBench (table)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-260174f5a5690a7e51bdf14e210da8b4</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918265</data>
    </edge>
    <edge source="Ablation Study Results on DocBench (table)" target="Legal Domain">
      <data key="d6">10.0</data>
      <data key="d7">Entity Legal Domain belongs to Ablation Study Results on DocBench (table)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-260174f5a5690a7e51bdf14e210da8b4</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918265</data>
    </edge>
    <edge source="Ablation Study Results on DocBench (table)" target="News Domain">
      <data key="d6">10.0</data>
      <data key="d7">Entity News Domain belongs to Ablation Study Results on DocBench (table)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-260174f5a5690a7e51bdf14e210da8b4</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918266</data>
    </edge>
    <edge source="Ablation Study Results on DocBench (table)" target="Text Type">
      <data key="d6">10.0</data>
      <data key="d7">Entity Text Type belongs to Ablation Study Results on DocBench (table)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-260174f5a5690a7e51bdf14e210da8b4</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918267</data>
    </edge>
    <edge source="Ablation Study Results on DocBench (table)" target="Multimodal Type">
      <data key="d6">10.0</data>
      <data key="d7">Entity Multimodal Type belongs to Ablation Study Results on DocBench (table)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-260174f5a5690a7e51bdf14e210da8b4</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918268</data>
    </edge>
    <edge source="Ablation Study Results on DocBench (table)" target="Unannotated Type">
      <data key="d6">10.0</data>
      <data key="d7">Entity Unannotated Type belongs to Ablation Study Results on DocBench (table)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-260174f5a5690a7e51bdf14e210da8b4</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918268</data>
    </edge>
    <edge source="Ablation Study Results on DocBench (table)" target="70.0% Overall Score">
      <data key="d6">10.0</data>
      <data key="d7">Entity 70.0% Overall Score belongs to Ablation Study Results on DocBench (table)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-260174f5a5690a7e51bdf14e210da8b4</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918270</data>
    </edge>
    <edge source="Ablation Study Results on DocBench (table)" target="62.4% Overall Score">
      <data key="d6">10.0</data>
      <data key="d7">Entity 62.4% Overall Score belongs to Ablation Study Results on DocBench (table)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-260174f5a5690a7e51bdf14e210da8b4</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918271</data>
    </edge>
    <edge source="Ablation Study Results on DocBench (table)" target="60.0% Overall Score">
      <data key="d6">10.0</data>
      <data key="d7">Entity 60.0% Overall Score belongs to Ablation Study Results on DocBench (table)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-260174f5a5690a7e51bdf14e210da8b4</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918271</data>
    </edge>
    <edge source="Multimodal Document with t-SNE Analysis (image)" target="t-SNE Plots">
      <data key="d6">10.0</data>
      <data key="d7">Entity t-SNE Plots belongs to Multimodal Document with t-SNE Analysis (image)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-76f41ba1f43d29b4684e8f0113c3efae</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918232</data>
    </edge>
    <edge source="Multimodal Document with t-SNE Analysis (image)" target="DAE Panel">
      <data key="d6">10.0</data>
      <data key="d7">Entity DAE Panel belongs to Multimodal Document with t-SNE Analysis (image)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-76f41ba1f43d29b4684e8f0113c3efae</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918233</data>
    </edge>
    <edge source="Multimodal Document with t-SNE Analysis (image)" target="Multi-Panel Figure Interpretation Case">
      <data key="d6">10.0</data>
      <data key="d7">Entity Multi-Panel Figure Interpretation Case belongs to Multimodal Document with t-SNE Analysis (image)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-76f41ba1f43d29b4684e8f0113c3efae</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918234</data>
    </edge>
    <edge source="Multimodal Document with t-SNE Analysis (image)" target="VAE Panel">
      <data key="d6">10.0</data>
      <data key="d7">Entity VAE Panel belongs to Multimodal Document with t-SNE Analysis (image)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-76f41ba1f43d29b4684e8f0113c3efae</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918234</data>
    </edge>
    <edge source="Multimodal Document with t-SNE Analysis (image)" target="Multimodal Document">
      <data key="d6">10.0</data>
      <data key="d7">Entity Multimodal Document belongs to Multimodal Document with t-SNE Analysis (image)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-76f41ba1f43d29b4684e8f0113c3efae</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918235</data>
    </edge>
    <edge source="Multimodal Document with t-SNE Analysis (image)" target="Question Prompt">
      <data key="d6">10.0</data>
      <data key="d7">Entity Question Prompt belongs to Multimodal Document with t-SNE Analysis (image)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-76f41ba1f43d29b4684e8f0113c3efae</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918237</data>
    </edge>
    <edge source="Novo Nordisk Financial Analysis Image (image)" target="Multimodel Document">
      <data key="d6">10.0</data>
      <data key="d7">Entity Multimodel Document belongs to Novo Nordisk Financial Analysis Image (image)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-fb95b7d9ca41e3520dd06a9228985544</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918248</data>
    </edge>
    <edge source="Novo Nordisk Financial Analysis Image (image)" target="Wages and Salaries">
      <data key="d6">10.0</data>
      <data key="d7">Entity Wages and Salaries belongs to Novo Nordisk Financial Analysis Image (image)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-fb95b7d9ca41e3520dd06a9228985544</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918249</data>
    </edge>
    <edge source="Novo Nordisk Financial Analysis Image (image)" target="Evidence Table">
      <data key="d6">10.0</data>
      <data key="d7">Entity Evidence Table belongs to Novo Nordisk Financial Analysis Image (image)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-fb95b7d9ca41e3520dd06a9228985544</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918250</data>
    </edge>
    <edge source="Novo Nordisk Financial Analysis Image (image)" target="26,778">
      <data key="d6">10.0</data>
      <data key="d7">Entity 26,778 belongs to Novo Nordisk Financial Analysis Image (image)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-fb95b7d9ca41e3520dd06a9228985544</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918251</data>
    </edge>
    <edge source="Novo Nordisk Financial Analysis Image (image)" target="2020">
      <data key="d6">10.0</data>
      <data key="d7">Entity 2020 belongs to Novo Nordisk Financial Analysis Image (image)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-fb95b7d9ca41e3520dd06a9228985544</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918251</data>
    </edge>
    <edge source="Novo Nordisk Financial Analysis Image (image)" target="2019">
      <data key="d6">10.0</data>
      <data key="d7">Entity 2019 belongs to Novo Nordisk Financial Analysis Image (image)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-fb95b7d9ca41e3520dd06a9228985544</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918252</data>
    </edge>
    <edge source="Novo Nordisk Financial Analysis Image (image)" target="2018">
      <data key="d6">10.0</data>
      <data key="d7">Entity 2018 belongs to Novo Nordisk Financial Analysis Image (image)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-fb95b7d9ca41e3520dd06a9228985544</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918253</data>
    </edge>
    <edge source="Novo Nordisk Financial Analysis Image (image)" target="Financial Document">
      <data key="d6">10.0</data>
      <data key="d7">Entity Financial Document belongs to Novo Nordisk Financial Analysis Image (image)</data>
      <data key="d8">belongs_to,contained_in,part_of</data>
      <data key="d9">chunk-fb95b7d9ca41e3520dd06a9228985544</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918253</data>
    </edge>
    <edge source="Mathematical Equation" target="Decompose">
      <data key="d6">1.0</data>
      <data key="d7">The Mathematical Equation illustrates the process of decomposition as it transforms a knowledge source into atomic content units.</data>
      <data key="d8">process,transformation</data>
      <data key="d9">chunk-88ebe14996977f568b531a51ff2b02e2</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918204</data>
    </edge>
    <edge source="Mathematical Equation" target="RAG-Anything Framework">
      <data key="d6">1.0</data>
      <data key="d7">The RAG-Anything Framework employs the principles illustrated in the Mathematical Equation to manage multimodal knowledge.</data>
      <data key="d8">application,representation</data>
      <data key="d9">chunk-88ebe14996977f568b531a51ff2b02e2</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918205</data>
    </edge>
    <edge source="Mathematical Equation" target="T">
      <data key="d6">1.0</data>
      <data key="d7">The Mathematical Equation formulates the construction of T, which is crucial for representing knowledge in multimodal contexts.</data>
      <data key="d8">function representation,knowledge construction</data>
      <data key="d9">chunk-11b9991810e9963c09e4017983a04dc3</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918233</data>
    </edge>
    <edge source="Mathematical Equation" target="Multimodal Retrieval Systems">
      <data key="d6">1.0</data>
      <data key="d7">The Mathematical Equation serves a crucial role in enhancing Multimodal Retrieval Systems by improving knowledge representation and retrieval efficiency.</data>
      <data key="d8">knowledge representation,retrieval efficiency</data>
      <data key="d9">chunk-11b9991810e9963c09e4017983a04dc3</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918234</data>
    </edge>
    <edge source="Mathematical Equation" target="Multimodal Vertex Entities">
      <data key="d6">1.0</data>
      <data key="d7">The Mathematical Equation expresses the construction and relevance of Multimodal Vertex Entities in knowledge graph frameworks.</data>
      <data key="d8">knowledge graph construction,theory application</data>
      <data key="d9">chunk-1b0413a81536bbb879f772d32b8c0dfe</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918271</data>
    </edge>
    <edge source="Mathematical Equation" target="Mathematical Analysis">
      <data key="d6">1.0</data>
      <data key="d7">The Mathematical Equation is a key focus of Mathematical Analysis, which explores its implications for knowledge graph construction.</data>
      <data key="d8">equation significance,theory application</data>
      <data key="d9">chunk-1b0413a81536bbb879f772d32b8c0dfe</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918273</data>
    </edge>
    <edge source="Decompose" target="RAG-Anything Framework">
      <data key="d6">1.0</data>
      <data key="d7">The RAG-Anything Framework utilizes the Decompose method to enhance multimodal knowledge indexing and retrieval.</data>
      <data key="d8">knowledge indexing,methodology</data>
      <data key="d9">chunk-88ebe14996977f568b531a51ff2b02e2</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918205</data>
    </edge>
    <edge source="Atomic Content Units" target="Knowledge Source">
      <data key="d6">2.0</data>
      <data key="d7">Atomic Content Units are characterized by modality type and raw content derived from the Knowledge Source during decomposition.&lt;SEP&gt;The Knowledge Source is transformed into Atomic Content Units during the decomposition process as outlined in the equation.</data>
      <data key="d8">abstraction,characterization,modality,transformation</data>
      <data key="d9">chunk-88ebe14996977f568b531a51ff2b02e2</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918204</data>
    </edge>
    <edge source="Atomic Content Units" target="Modality Type">
      <data key="d6">1.0</data>
      <data key="d7">The Modality Type characterizes each Atomic Content Unit, indicating the form of content it represents.</data>
      <data key="d8">characterization,classification</data>
      <data key="d9">chunk-88ebe14996977f568b531a51ff2b02e2</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918205</data>
    </edge>
    <edge source="RAG-Anything Framework" target="Cross-Modal Retrieval">
      <data key="d6">1.0</data>
      <data key="d7">The RAG-Anything Framework enhances Cross-Modal Retrieval by transforming heterogeneous knowledge into structured formats.</data>
      <data key="d8">integration,knowledge transformation</data>
      <data key="d9">chunk-2d23b106580d44e93ef9110173f0cfaf</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918207</data>
    </edge>
    <edge source="Equation" target="Graph Structure">
      <data key="d6">1.0</data>
      <data key="d7">The equation represents a mapping that leads to various interpretations within graph structures.</data>
      <data key="d8">conceptual analysis,mathematical representation</data>
      <data key="d9">chunk-2d23b106580d44e93ef9110173f0cfaf</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918206</data>
    </edge>
    <edge source="Graph Structure" target="Structured Knowledge Representation">
      <data key="d6">1.0</data>
      <data key="d7">Structured Knowledge Representation emphasizes organizing knowledge in the context of Graph Structures for better usability within AI environments.</data>
      <data key="d8">AI systems,knowledge organization</data>
      <data key="d9">chunk-2d23b106580d44e93ef9110173f0cfaf</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918207</data>
    </edge>
    <edge source="Graph Structure" target="Mapping of Content Unit">
      <data key="d6">1.0</data>
      <data key="d7">The Mapping of Content Unit into Graph Structure facilitates the integration of textual and non-textual information.</data>
      <data key="d8">knowledge integration,relationship representation</data>
      <data key="d9">chunk-2d23b106580d44e93ef9110173f0cfaf</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918208</data>
    </edge>
    <edge source="Set of Vertices" target="Set of Edges">
      <data key="d6">1.0</data>
      <data key="d7">The Set of Vertices and Set of Edges represent the fundamental components of a graph in mathematical analysis.</data>
      <data key="d8">graph theory,relationships</data>
      <data key="d9">chunk-2d23b106580d44e93ef9110173f0cfaf</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918206</data>
    </edge>
    <edge source="Set of Vertices" target="Function R">
      <data key="d6">1.0</data>
      <data key="d7">Function R transforms the generated textual description into a Set of Vertices that represents entities in the graph.</data>
      <data key="d8">graph definition,transformation</data>
      <data key="d9">chunk-2d23b106580d44e93ef9110173f0cfaf</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918208</data>
    </edge>
    <edge source="Generated Textual Description" target="Dual-Graph Methodology">
      <data key="d6">1.0</data>
      <data key="d7">The Generated Textual Description is indexed under the Dual-Graph Methodology to facilitate understanding of its context.</data>
      <data key="d8">indexing,semantic encapsulation</data>
      <data key="d9">chunk-2d23b106580d44e93ef9110173f0cfaf</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918207</data>
    </edge>
    <edge source="DocBench Accuracy" target="RAGAnything">
      <data key="d6">1.0</data>
      <data key="d7">RAGAnything shows superior performance in the DocBench Accuracy chart compared to MMGraphRAG.</data>
      <data key="d8">data comparison,performance metric</data>
      <data key="d9">chunk-0ed9b2ed0a4bc6dedef04c5c1a224400</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918212</data>
    </edge>
    <edge source="DocBench Accuracy" target="DocBench QA Counts">
      <data key="d6">1.0</data>
      <data key="d7">DocBench QA Counts relates to the performance evaluation seen in DocBench Accuracy.</data>
      <data key="d8">metric representation,visual data</data>
      <data key="d9">chunk-0ed9b2ed0a4bc6dedef04c5c1a224400</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918213</data>
    </edge>
    <edge source="DocBench Accuracy" target="Page Range Categories">
      <data key="d6">1.0</data>
      <data key="d7">The 'DocBench Accuracy' utilizes page range categories as its x-axis to display performance metrics.</data>
      <data key="d8">data representation,metric comparison</data>
      <data key="d9">chunk-0ed9b2ed0a4bc6dedef04c5c1a224400</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918215</data>
    </edge>
    <edge source="RAGAnything" target="DocBench Dataset">
      <data key="d6">1.0</data>
      <data key="d7">RAGAnything's performance is analyzed within the context of the DocBench Dataset, highlighting its effectiveness across various tasks.</data>
      <data key="d8">evaluation,performance analysis</data>
      <data key="d9">chunk-c74c755aa200873b54b093c4aef3485e</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918216</data>
    </edge>
    <edge source="RAGAnything" target="Finance">
      <data key="d6">1.0</data>
      <data key="d7">RAGAnything achieved high accuracy in the Finance domain, indicating its effectiveness in that category.</data>
      <data key="d8">competitive accuracy,domain excellence</data>
      <data key="d9">chunk-c74c755aa200873b54b093c4aef3485e</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918217</data>
    </edge>
    <edge source="RAGAnything" target="News">
      <data key="d6">1.0</data>
      <data key="d7">RAGAnything's performance in the News domain demonstrates its capability in handling news articles effectively.</data>
      <data key="d8">competitive accuracy,domain excellence</data>
      <data key="d9">chunk-c74c755aa200873b54b093c4aef3485e</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918218</data>
    </edge>
    <edge source="RAGAnything" target="Overall Accuracy">
      <data key="d6">1.0</data>
      <data key="d7">RAGAnything's strong showing in the Multimodal category contributes positively to the overall accuracy metrics in the study.</data>
      <data key="d8">evaluation metrics,performance highlight</data>
      <data key="d9">chunk-c74c755aa200873b54b093c4aef3485e</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918219</data>
    </edge>
    <edge source="MMLongBench Accuracy" target="MMLongBench QA Counts">
      <data key="d6">1.0</data>
      <data key="d7">MMLongBench Accuracy represents similar data metrics as shown in MMLongBench QA Counts.</data>
      <data key="d8">metric representation,visual data</data>
      <data key="d9">chunk-0ed9b2ed0a4bc6dedef04c5c1a224400</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918213</data>
    </edge>
    <edge source="MMLongBench Accuracy" target="Page Range Categories">
      <data key="d6">1.0</data>
      <data key="d7">The 'MMLongBench Accuracy' similarly uses page range categories as its x-axis for metrics.</data>
      <data key="d8">data representation,metric comparison</data>
      <data key="d9">chunk-0ed9b2ed0a4bc6dedef04c5c1a224400</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918214</data>
    </edge>
    <edge source="Retrieval and Response Generation" target="Vector Database">
      <data key="d6">1.0</data>
      <data key="d7">Retrieval and Response Generation utilizes the Vector Database as the source for information retrieval during the querying process.</data>
      <data key="d8">data source,information retrieval</data>
      <data key="d9">chunk-92698bb9a611d4820a4578e37e74e670</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918226</data>
    </edge>
    <edge source="Retrieval and Response Generation" target="Query Processing">
      <data key="d6">1.0</data>
      <data key="d7">Retrieval and Response Generation is a part of the Query Processing that outlines the flow from query input to response generation.</data>
      <data key="d8">information extraction,process flow</data>
      <data key="d9">chunk-92698bb9a611d4820a4578e37e74e670</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918227</data>
    </edge>
    <edge source="Multi-Panel Figure Interpretation Case" target="t-SNE Plots">
      <data key="d6">1.0</data>
      <data key="d7">The Multi-Panel Figure Interpretation Case utilizes t-SNE plots to analyze separation patterns in the style space.</data>
      <data key="d8">data representation,visualization technique</data>
      <data key="d9">chunk-76f41ba1f43d29b4684e8f0113c3efae</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918230</data>
    </edge>
    <edge source="Multi-Panel Figure Interpretation Case" target="DAE Panel">
      <data key="d6">1.0</data>
      <data key="d7">The DAE Panel serves as evidence in the Multi-Panel Figure Interpretation Case due to its distinct cluster separation.</data>
      <data key="d8">clarity,evidence</data>
      <data key="d9">chunk-76f41ba1f43d29b4684e8f0113c3efae</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918230</data>
    </edge>
    <edge source="Multi-Panel Figure Interpretation Case" target="VAE Panel">
      <data key="d6">1.0</data>
      <data key="d7">The VAE Panel is part of the Multi-Panel Figure Interpretation Case illustrating a lesser clarity of cluster separation.</data>
      <data key="d8">clarity,evidence</data>
      <data key="d9">chunk-76f41ba1f43d29b4684e8f0113c3efae</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918231</data>
    </edge>
    <edge source="Multi-Panel Figure Interpretation Case" target="Multimodal Document">
      <data key="d6">1.0</data>
      <data key="d7">The Multimodal Document provides context and serves as a source of evidence for the insights derived from the Multi-Panel Figure Interpretation Case.</data>
      <data key="d8">context,evidence source</data>
      <data key="d9">chunk-76f41ba1f43d29b4684e8f0113c3efae</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918232</data>
    </edge>
    <edge source="DAE Panel" target="VAE Panel">
      <data key="d6">1.0</data>
      <data key="d7">The DAE Panel shows clearer separation between clusters compared to the VAE Panel.</data>
      <data key="d8">cluster separation,comparison</data>
      <data key="d9">chunk-76f41ba1f43d29b4684e8f0113c3efae</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918230</data>
    </edge>
    <edge source="DAE Panel" target="Question Prompt">
      <data key="d6">2.0</data>
      <data key="d7">The DAE Panel provides evidence related to the inquiry presented in the Question Prompt.&lt;SEP&gt;The Question Prompt inquires about the clarity of cluster separation in the DAE Panel.</data>
      <data key="d8">cluster separation,evidence,inquiry,inquiry context</data>
      <data key="d9">chunk-76f41ba1f43d29b4684e8f0113c3efae</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918231</data>
    </edge>
    <edge source="VAE Panel" target="Question Prompt">
      <data key="d6">2.0</data>
      <data key="d7">The VAE Panel provides evidence related to the inquiry presented in the Question Prompt.&lt;SEP&gt;The Question Prompt inquires about the clarity of cluster separation in the VAE Panel.</data>
      <data key="d8">cluster separation,evidence,inquiry,inquiry context</data>
      <data key="d9">chunk-76f41ba1f43d29b4684e8f0113c3efae</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918232</data>
    </edge>
    <edge source="emb" target="V">
      <data key="d6">1.0</data>
      <data key="d7">The function emb(s) generates embeddings for the set V, which consists of graph entities.</data>
      <data key="d8">embedding function,entity representation</data>
      <data key="d9">chunk-11b9991810e9963c09e4017983a04dc3</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918234</data>
    </edge>
    <edge source="emb" target="E">
      <data key="d6">1.0</data>
      <data key="d7">The function emb(s) provides embeddings for the relationships in set E, indicating their connections in the embedding space.</data>
      <data key="d8">embedding function,relationship representation</data>
      <data key="d9">chunk-11b9991810e9963c09e4017983a04dc3</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918234</data>
    </edge>
    <edge source="emb" target="Similarity Searches">
      <data key="d6">1.0</data>
      <data key="d7">Similarity Searches utilize the embeddings generated by the function emb(s) to perform effective retrieval in the embedding space.</data>
      <data key="d8">embedding application,search technique</data>
      <data key="d9">chunk-11b9991810e9963c09e4017983a04dc3</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918235</data>
    </edge>
    <edge source="V" target="c">
      <data key="d6">1.0</data>
      <data key="d7">The set V is integrated with c_{j_{j}}, representing different content modalities for comprehensive analysis.</data>
      <data key="d8">component integration,multimodal analysis</data>
      <data key="d9">chunk-11b9991810e9963c09e4017983a04dc3</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918234</data>
    </edge>
    <edge source="c" target="Atom Content Chunks">
      <data key="d6">1.0</data>
      <data key="d7">Atom Content Chunks are described by the variable c_{j_{j}}, indicating their role in integrating various content types in the mathematical model.</data>
      <data key="d8">component definition,multimodal integration</data>
      <data key="d9">chunk-11b9991810e9963c09e4017983a04dc3</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918235</data>
    </edge>
    <edge source="Multimodel Document" target="Evidence Table">
      <data key="d6">1.0</data>
      <data key="d7">The Evidence Table is a highlighted section within the Multimodel Document that focuses on employee costs.</data>
      <data key="d8">content representation,financial document</data>
      <data key="d9">chunk-fb95b7d9ca41e3520dd06a9228985544</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918245</data>
    </edge>
    <edge source="Multimodel Document" target="Financial Document">
      <data key="d6">1.0</data>
      <data key="d7">The Multimodel Document visually presents the Financial Document with highlights to guide the viewer's focus on specific areas.</data>
      <data key="d8">content representation,detailed analysis</data>
      <data key="d9">chunk-fb95b7d9ca41e3520dd06a9228985544</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918248</data>
    </edge>
    <edge source="Evidence Table" target="Wages and Salaries">
      <data key="d6">1.0</data>
      <data key="d7">Wages and Salaries are prominently featured in the Evidence Table, indicating a specific financial figure for 2020.</data>
      <data key="d8">financial data,key information</data>
      <data key="d9">chunk-fb95b7d9ca41e3520dd06a9228985544</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918246</data>
    </edge>
    <edge source="Evidence Table" target="2019">
      <data key="d6">1.0</data>
      <data key="d7">The Evidence Table includes data for 2019 for comparative purposes alongside the current employee costs.</data>
      <data key="d8">comparative analysis,historical data</data>
      <data key="d9">chunk-fb95b7d9ca41e3520dd06a9228985544</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918247</data>
    </edge>
    <edge source="Evidence Table" target="2018">
      <data key="d6">1.0</data>
      <data key="d7">The Evidence Table includes data for 2018 for comparative purposes alongside the current employee costs.</data>
      <data key="d8">comparative analysis,historical data</data>
      <data key="d9">chunk-fb95b7d9ca41e3520dd06a9228985544</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918248</data>
    </edge>
    <edge source="Wages and Salaries" target="26,778">
      <data key="d6">1.0</data>
      <data key="d7">26,778 is the highlighted amount corresponding to Wages and Salaries for the year 2020 in the Evidence Table.</data>
      <data key="d8">financial value,specific amount</data>
      <data key="d9">chunk-fb95b7d9ca41e3520dd06a9228985544</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918247</data>
    </edge>
    <edge source="Ablation Study" target="Chunk-Only">
      <data key="d6">1.0</data>
      <data key="d7">The ablation study compares the "Chunk-only" method to other retrieval methods.</data>
      <data key="d8">method comparison,performance analysis</data>
      <data key="d9">chunk-260174f5a5690a7e51bdf14e210da8b4</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918253</data>
    </edge>
    <edge source="Ablation Study" target="W/O Reranker">
      <data key="d6">1.0</data>
      <data key="d7">The ablation study compares the "w/o Reranker" method to other retrieval methods.</data>
      <data key="d8">method comparison,performance analysis</data>
      <data key="d9">chunk-260174f5a5690a7e51bdf14e210da8b4</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918253</data>
    </edge>
    <edge source="Chunk-Only" target="60.0% Overall Score">
      <data key="d6">1.0</data>
      <data key="d7">The performance of the "Chunk-only" method is represented by its overall score of 60.0%.</data>
      <data key="d8">evaluation results,performance outcome</data>
      <data key="d9">chunk-260174f5a5690a7e51bdf14e210da8b4</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918258</data>
    </edge>
    <edge source="W/O Reranker" target="62.4% Overall Score">
      <data key="d6">1.0</data>
      <data key="d7">The performance of the "w/o Reranker" method is depicted through its overall score of 62.4%.</data>
      <data key="d8">evaluation results,performance outcome</data>
      <data key="d9">chunk-260174f5a5690a7e51bdf14e210da8b4</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918258</data>
    </edge>
    <edge source="Performance Metrics" target="Academic Domain">
      <data key="d6">1.0</data>
      <data key="d7">The Academic domain performance metrics are part of the overall evaluation in the ablation study.</data>
      <data key="d8">domain testing,retrieval evaluation</data>
      <data key="d9">chunk-260174f5a5690a7e51bdf14e210da8b4</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918256</data>
    </edge>
    <edge source="Performance Metrics" target="Financial Domain">
      <data key="d6">1.0</data>
      <data key="d7">The Financial domain performance metrics are part of the overall evaluation in the ablation study.</data>
      <data key="d8">domain testing,retrieval evaluation</data>
      <data key="d9">chunk-260174f5a5690a7e51bdf14e210da8b4</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918257</data>
    </edge>
    <edge source="Performance Metrics" target="Governmental Domain">
      <data key="d6">1.0</data>
      <data key="d7">The Governmental domain performance metrics are part of the overall evaluation in the ablation study.</data>
      <data key="d8">domain testing,retrieval evaluation</data>
      <data key="d9">chunk-260174f5a5690a7e51bdf14e210da8b4</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918258</data>
    </edge>
    <edge source="Performance Metrics" target="Legal Domain">
      <data key="d6">1.0</data>
      <data key="d7">The Legal domain performance metrics are part of the overall evaluation in the ablation study.</data>
      <data key="d8">domain testing,retrieval evaluation</data>
      <data key="d9">chunk-260174f5a5690a7e51bdf14e210da8b4</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918259</data>
    </edge>
    <edge source="Performance Metrics" target="News Domain">
      <data key="d6">1.0</data>
      <data key="d7">The News domain performance metrics are part of the overall evaluation in the ablation study.</data>
      <data key="d8">domain testing,retrieval evaluation</data>
      <data key="d9">chunk-260174f5a5690a7e51bdf14e210da8b4</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918259</data>
    </edge>
    <edge source="Performance Metrics" target="Text Type">
      <data key="d6">1.0</data>
      <data key="d7">The Text Type metrics provide classification data for evaluation in the ablation study.</data>
      <data key="d8">classification analysis,retrieval evaluation</data>
      <data key="d9">chunk-260174f5a5690a7e51bdf14e210da8b4</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918260</data>
    </edge>
    <edge source="Performance Metrics" target="Multimodal Type">
      <data key="d6">1.0</data>
      <data key="d7">The Multimodal Type metrics provide classification data that showcases retrieval performance.</data>
      <data key="d8">classification analysis,retrieval evaluation</data>
      <data key="d9">chunk-260174f5a5690a7e51bdf14e210da8b4</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918261</data>
    </edge>
    <edge source="Performance Metrics" target="Unannotated Type">
      <data key="d6">1.0</data>
      <data key="d7">The Unannotated Type metrics provide classification data relevant to the evaluation process.</data>
      <data key="d8">classification analysis,retrieval evaluation</data>
      <data key="d9">chunk-260174f5a5690a7e51bdf14e210da8b4</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918262</data>
    </edge>
    <edge source="Response" target="VLM">
      <data key="d6">1.0</data>
      <data key="d7">The VLM generates a response based on the query and its associated contextual data.</data>
      <data key="d8">function,output generation</data>
      <data key="d9">chunk-3fde5aaf8f821b4be0b48eb6bfb98337</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918267</data>
    </edge>
    <edge source="Response" target="P(q)">
      <data key="d6">1.0</data>
      <data key="d7">The structured textual context P(q) supports the generation of the Response by the VLM.</data>
      <data key="d8">contextual support,output generation</data>
      <data key="d9">chunk-3fde5aaf8f821b4be0b48eb6bfb98337</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918268</data>
    </edge>
    <edge source="Response" target="V*(q)">
      <data key="d6">1.0</data>
      <data key="d7">The dereferenced visual content V*(q) contributes to the Response generated by the VLM.</data>
      <data key="d8">output generation,visual support</data>
      <data key="d9">chunk-3fde5aaf8f821b4be0b48eb6bfb98337</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918268</data>
    </edge>
    <edge source="Response" target="Hybrid Retrieval Mechanisms">
      <data key="d6">1.0</data>
      <data key="d7">The Response generated employs hybrid retrieval mechanisms to effectively integrate multiple modalities in answering queries.</data>
      <data key="d8">integrated approach,system function</data>
      <data key="d9">chunk-3fde5aaf8f821b4be0b48eb6bfb98337</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918269</data>
    </edge>
    <edge source="VLM" target="q">
      <data key="d6">1.0</data>
      <data key="d7">The query q is processed by the VLM to generate an appropriate response.</data>
      <data key="d8">input,processing</data>
      <data key="d9">chunk-3fde5aaf8f821b4be0b48eb6bfb98337</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918268</data>
    </edge>
    <edge source="Hybrid Retrieval Mechanisms" target="Advanced AI Systems">
      <data key="d6">1.0</data>
      <data key="d7">Hybrid retrieval mechanisms are utilized in advanced AI systems to synthesize multimodal knowledge for improved question-answering.</data>
      <data key="d8">application,knowledge synthesis</data>
      <data key="d9">chunk-3fde5aaf8f821b4be0b48eb6bfb98337</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918268</data>
    </edge>
    <edge source="Multimodal Vertex Entities" target="Intra-Chunk Entities">
      <data key="d6">1.0</data>
      <data key="d7">Multimodal Vertex Entities include Intra-Chunk Entities, enhancing the richness of the graph structure.</data>
      <data key="d8">data structure,enrichment</data>
      <data key="d9">chunk-1b0413a81536bbb879f772d32b8c0dfe</data>
      <data key="d10">research_paper.pdf</data>
      <data key="d11">1760918272</data>
    </edge>
  </graph>
</graphml>
